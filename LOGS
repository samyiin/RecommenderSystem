The content based recommender system have the following architecture:
CONTENT ANALYZER
    This module will convert the information source into represented items

PROFILE LEARNER
    This module have two phase:
    (1) The learning phase: For an user u, it will learn the user's rating for each represented items, and create a user
    profile.
    (2) The online phase: For every new item that feeds to the user, it will record the user's feedback/rating to the
    item, and update the user's profile.

FILTER COMPONENT
    This module will receive a set of represented items, and the user profile of user u, and recommend to u a list of
    recommendations.
========================================================================================================================
FIRST EDITION:
A refined version of TF-IDF: different representation of information source and user profile.

CONTENT ANALYZER
In our first edition, we will use Aviv's semantic scholar data as information source, and embed it into 1536 dimension-
vector using openAI's 'text-embedding-ada-002' embedding method. And we will not use a database, we will use pandas df
as our database for now.

PROFILE LEARNER
In the first edition, we will represent user profile also as 1536 dimension-vector, so it will be easier for FILTER
COMPONENT, We will take every paper the user ever liked in the training phase, and average their embeddings. So for a
user.
For the user feedback, we simplified as if a user read a paper and like it, he will add it to his read papers, and
consequently his profile embedding will change, and we will update this in the profile database.

FILTER COMPONENT
In the first edition, the priority of a recommending a paper to a user will be determined by the cosine similarity of
a user profile and the represented item, both 1536-dimension-vectors.

========================================================================================================================
Problems:
1. I need a database: I need to input all the papers into a database, so that given an author, I can find all his works
and all the papers he cited.
    1.1. Once I have a database, the interface remains unchanged, but the "_save_file_to_database" and "get_xx_by_id"
         will change in both databases. (maybe even the whole init?)
    1.2. the query database will change too.
2. Still haven't decided how to embed a paper, now we have the metadata, but not always have the paper pdf/txt. Are we
going to embed the metadata?

========================================================================================================================
OTHER POSSIBILITIES
For CONTENT ANALYZER, there are other ways to represent a paper:
Keyword-based systems
Semantic Analysis by ontologies (openAI should be this kind)
Semantic Analysis by encyclopedic knowledge source

For PROFILE LEARNER (first of all, we can try to complete the feedback functionality)
Probabilistic methods, like Naive Bayes
Relevance Feedback method: the Rocchio's algorithm
machine learning techniques to learn the classifier of user profile.

Then once the user profile is some sort of a classifier, we can just feed the user the most relevant content in the
FILTER COMPONENT phase. (According to how the classifier is defined.

========================================================================================================================
SECOND_EDITION
There are three changes:
1. The paper will be represented by a new way, with abstract and summary so that we can embed the papers.
    1.1 The paper topics will be confined to a certain topic, say machine learning
2. We will use a database to process instead of taking sample files to process.
3. We will add more ways of recommending beside from cosine similarity, such as order by citation.
4. We added recommend by linear combination of rankings for cosine similarity, citation, reference and infulencial
    citation
5.

========================================================================================================================
today I decided to make a little modifications on the API of database. The database classes should act as a high level
interface for interacting with the database, so it shouldn't be the caller's responsibility to decode the stored data.
So from now on, the query_database method will return a decoded pandas dataframe.

And also recommender should act as a high level interface for tasks. It should encapsulate the task, do the database
query internally. So it shouldn't be the caller's responsibility to  first get author's embedding, and the told the
recommender to recommend based on the embedding. If you want you can build a tool box for recommender to use, such as
strategies for recommending.

Thanks to find usages, I gets to change the api without too much effort.

========================================================================================================================
1. run remotely on aqua computer
2. run on a normal computer - github