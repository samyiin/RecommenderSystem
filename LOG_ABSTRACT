=======2023.May.6=======
The first goal is to set up a simple content based recommender system.
My first approach is:
(1) convert a research paper pdf into meta datas
(2) embed these meta datas into vector using openAI's embedding service
(3) recommend to user using similarity of the vectors

Spent the whole day trying to use grobid, didn't work in the end. look at MY_LOG under grobid.

The next possibility is GPT model.

token for github 90 days: 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

=======2023.May.7=======
Today tried to use GPT2, succeeded,but the generated text is not good, and the response is really slow, so give up.
Before I set up GPT2, I wrote a program to read pdf file into single string py script,
Next thing to try is: CERMINE, Core, CrossRef Metadata Search, ParsCit, Neural-ParsCit
Possibility for summarization: TextRank, Gensim
I will start with CERMINE. It works!!! see under paper_reader/CERMINE/MY_LOG
Now I have CERMINE "zone" file, now I need to see how to embed it. Hypothetically I can first parse it into a tabel, but
I will try first just pass it in as a zone file (or string)

Now I will start with openAI

=======2023.May.9=======
Today I set up the framework of embedder: the embedder will first split long text into segments because of the embedding
 length constrain, then it will use pooling methods to get the embed for the whole paper.
Another possibility could be summarize the paper, and then embed the summarized text. But there is no reason to use that
for now.

I think I should set a naming convention for the project:
package: ??? in the future all cap?
py file: all lower case connect with _
class name: camel convention, cap all first word: e.g. PaperParser
class instance: all lower case connect with _


=======2023.May.10=======
Today I finished the setting up of embedding, so now at least I have one way of embedding the papers. Tomorrow I will
try to set up the database for this first edition.
Also a lot of stuff can be done in this stage later: try to find a free alternative of embedding?
    nltk, sklearn, tf-idf etc.
For test we can set up fake embeddings too, just for testing.

Next three steps:
(1) How do I setup a link between papers and it's embeddings?
    I need a database, columns?
(2) How to setup the content based RS? What about based on citation and stuff?
    Maybe citation can considered as "item based collaborative filtering" (if you cited a paper means you like it), now
    let's first use cosine similarity for content based.
(3) set up test files, test paper_reader, embedder (not urgent?)

If want to transplant this program, I will need to change CERMINE's file paths, so I will first decide how do I access
research papers. So far I am thinking, setup a database, with id, name, <content>, embedding. But this requires further
 modify the parse method of paper reader(not the focus in this round). How about id, cerminezone_content, embedding?

This is some more findings:
this one seems to used Doc2Vec instead of openAI (still content based), semantic scholar
https://github.com/DeviantPadam/ResearchPaperScholarlyArticlesRecSystem
this one seems to use hybrid, seems to be item based, semantic scholar
https://github.com/NagaJanakiDwadasi/ResearchPaperRecommendationSystem
content based (and a lot more, basically the same idea, embed and similarity)
https://github.com/EnergizeStatistics/research-paper-recommender/blob/master/main.ipynb
A seemingly intergrated one
https://github.com/Muelli21/RecommendingPapers

Interesting thoughts: search engine or recommender system?

Today, reformat the pipeline. Use a "interact with database" approach. Also reformat the paper reader.

Change of plans, today we first try to read aviv's data.

=======2023.May.15=======
Thanks to Shlomo, solved the download data problem. So I will start to use Aviv's data for now.




