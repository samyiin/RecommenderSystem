{"content": "<document>\n  <zone label=\"MET_BIB_INFO\">Neural Collaborative Filtering\u2217</zone>\n  <zone label=\"MET_AFFILIATION\">Xiangnan He\nNational University of\nSingapore, Singapore\nxiangnanhe@gmail.com\nLiqiang Nie\nShandong University\nChina\nnieliqiang@gmail.com\nLizi Liao\nNational University of\nSingapore, Singapore\nliaolizi.llz@gmail.com\nXia Hu\nTexas A&amp;M University\nUSA\nhu@cse.tamu.edu\nHanwang Zhang\nColumbia University\nUSA\nhanwangzhang@gmail.com\nTat-Seng Chua\nNational University of\nSingapore, Singapore\ndcscts@nus.edu.sg</zone>\n  <zone label=\"MET_ABSTRACT\">ABSTRACT\nIn recent years, deep neural networks have yielded immense\nsuccess on speech recognition, computer vision and natural\nlanguage processing. However, the exploration of deep neural\nnetworks on recommender systems has received relatively\nless scrutiny. In this work, we strive to develop techniques\nbased on neural networks to tackle the key problem in recommendation\n- collaborative filtering - on the basis of\nimplicit feedback.\nAlthough some recent work has employed deep learning\nfor recommendation, they primarily used it to model auxiliary\ninformation, such as textual descriptions of items and\nacoustic features of musics. When it comes to model the key\nfactor in collaborative filtering - the interaction between\nuser and item features, they still resorted to matrix factorization\nand applied an inner product on the latent features\nof users and items.\nBy replacing the inner product with a neural architecture\nthat can learn an arbitrary function from data, we present\na general framework named NCF, short for Neural networkbased\nCollaborative Filtering. NCF is generic and can express\nand generalize matrix factorization under its framework.\nTo supercharge NCF modelling with non-linearities,\nwe propose to leverage a multi-layer perceptron to learn the\nuser-item interaction function. Extensive experiments on\ntwo real-world datasets show significant improvements of our\nproposed NCF framework over the state-of-the-art methods.\nEmpirical evidence shows that using deeper layers of neural\nnetworks offers better recommendation performance.</zone>\n  <zone label=\"MET_KEYWORDS\">Keywords</zone>\n  <zone label=\"MET_AFFILIATION\">Collaborative Filtering, Neural Networks, Deep Learning,\nMatrix Factorization, Implicit Feedback\n\u2217NExT research is supported by the National Research\nFoundation, Prime Minister's Office, Singapore under its\nIRC@SG Funding Initiative.</zone>\n  <zone label=\"MET_BIB_INFO\">c 2017 International World Wide Web Conference Committee\n(IW3C2), published under Creative Commons CC BY 4.0 License.\nWWW 2017, April 3-7, 2017, Perth, Australia.\nACM 978-1-4503-4913-0/17/04.\nhttp://dx.doi.org/10.1145/3038912.3052569</zone>\n  <zone label=\"GEN_OTHER\">.\n173</zone>\n  <zone label=\"BODY_HEADING\">1. INTRODUCTION</zone>\n  <zone label=\"BODY_CONTENT\">In the era of information explosion, recommender systems\nplay a pivotal role in alleviating information overload, having\nbeen widely adopted by many online services, including\nE-commerce, online news and social media sites. The key to\na personalized recommender system is in modelling users'\npreference on items based on their past interactions (e.g.,\nratings and clicks), known as collaborative filtering [31, 46].\nAmong the various collaborative filtering techniques, matrix\nfactorization (MF) [14, 21] is the most popular one, which\nprojects users and items into a shared latent space, using\na vector of latent features to represent a user or an item.\nThereafter a user's interaction on an item is modelled as the\ninner product of their latent vectors.\nPopularized by the Netflix Prize, MF has become the de\nfacto approach to latent factor model-based recommendation.\nMuch research effort has been devoted to enhancing\nMF, such as integrating it with neighbor-based models [21],\ncombining it with topic models of item content [38], and extending\nit to factorization machines [26] for a generic modelling\nof features. Despite the effectiveness of MF for collaborative\nfiltering, it is well-known that its performance can be\nhindered by the simple choice of the interaction function inner\nproduct. For example, for the task of rating prediction\non explicit feedback, it is well known that the performance\nof the MF model can be improved by incorporating user\nand item bias terms into the interaction function1. While\nit seems to be just a trivial tweak for the inner product\noperator [14], it points to the positive effect of designing a\nbetter, dedicated interaction function for modelling the latent\nfeature interactions between users and items. The inner\nproduct, which simply combines the multiplication of latent\nfeatures linearly, may not be sufficient to capture the complex\nstructure of user interaction data.\nThis paper explores the use of deep neural networks for\nlearning the interaction function from data, rather than a\nhandcraft that has been done by many previous work [18,\n21]. The neural network has been proven to be capable of\napproximating any continuous function [17], and more recently\ndeep neural networks (DNNs) have been found to be\neffective in several domains, ranging from computer vision,\nspeech recognition, to text processing [5, 10, 15, 47]. However,\nthere is relatively little work on employing DNNs for\nrecommendation in contrast to the vast amount of literature\n1http://alex.smola.org/teaching/berkeley2012/slides/8_\nRecommender.pdf\non MF methods. Although some recent advances [37, 38,\n45] have applied DNNs to recommendation tasks and shown\npromising results, they mostly used DNNs to model auxiliary\ninformation, such as textual description of items, audio\nfeatures of musics, and visual content of images. With regards\nto modelling the key collaborative filtering effect, they\nstill resorted to MF, combining user and item latent features\nusing an inner product.\nThis work addresses the aforementioned research problems\nby formalizing a neural network modelling approach for\ncollaborative filtering. We focus on implicit feedback, which\nindirectly reflects users' preference through behaviours like\nwatching videos, purchasing products and clicking items.\nCompared to explicit feedback (i.e., ratings and reviews),\nimplicit feedback can be tracked automatically and is thus\nmuch easier to collect for content providers. However, it is\nmore challenging to utilize, since user satisfaction is not observed\nand there is a natural scarcity of negative feedback.\nIn this paper, we explore the central theme of how to utilize\nDNNs to model noisy implicit feedback signals.\nThe main contributions of this work are as follows.\n1. We present a neural network architecture to model\nlatent features of users and items and devise a general\nframework NCF for collaborative filtering based\non neural networks.\n2. We show that MF can be interpreted as a specialization\nof NCF and utilize a multi-layer perceptron to endow\nNCF modelling with a high level of non-linearities.\n3. We perform extensive experiments on two real-world\ndatasets to demonstrate the effectiveness of our NCF\napproaches and the promise of deep learning for collaborative\nfiltering.\n2.</zone>\n  <zone label=\"BODY_HEADING\">PRELIMINARIES</zone>\n  <zone label=\"BODY_CONTENT\">We first formalize the problem and discuss existing solutions\nfor collaborative filtering with implicit feedback. We\nthen shortly recapitulate the widely used MF model, highlighting\nits limitation caused by using an inner product.\n2.1</zone>\n  <zone label=\"BODY_HEADING\">Learning from Implicit Data</zone>\n  <zone label=\"BODY_CONTENT\">Let M and N denote the number of users and items,\nrespectively. We define the user-item interaction matrix\nY \u2208 RM\u00d7N from users' implicit feedback as,\nyui =\n1, if interaction (user u, item i) is observed;\n0, otherwise.\n(1)\nHere a value of 1 for yui indicates that there is an interaction\nbetween user u and item i; however, it does not mean u\nactually likes i. Similarly, a value of 0 does not necessarily\nmean u does not like i, it can be that the user is not aware\nof the item. This poses challenges in learning from implicit\ndata, since it provides only noisy signals about users' preference.\nWhile observed entries at least reflect users' interest\non items, the unobserved entries can be just missing data\nand there is a natural scarcity of negative feedback.\nThe recommendation problem with implicit feedback is\nformulated as the problem of estimating the scores of unobserved\nentries in Y, which are used for ranking the items.\nModel-based approaches assume that data can be generated\n(or described) by an underlying model. Formally, they can\nbe abstracted as learning y\u02c6ui = f (u, i|\u0398), where y\u02c6ui denotes</zone>\n  <zone label=\"GEN_OTHER\">174</zone>\n  <zone label=\"BODY_CONTENT\">(a) user-item matrix\n(b) user latent space\nFigure 1: An example illustrates MF's limitation.\nFrom data matrix (a), u4 is most similar to u1, followed\nby u3, and lastly u2. However in the latent\nspace (b), placing p4 closest to p1 makes p4 closer to\np2 than p3, incurring a large ranking loss.\nthe predicted score of interaction yui, \u0398 denotes model parameters,\nand f denotes the function that maps model parameters\nto the predicted score (which we term as an interaction\nfunction).\nTo estimate parameters \u0398, existing approaches generally\nfollow the machine learning paradigm that optimizes an objective\nfunction. Two types of objective functions are most\ncommonly used in literature - pointwise loss [14, 19] and\npairwise loss [27, 33]. As a natural extension of abundant\nwork on explicit feedback [21, 46], methods on pointwise\nlearning usually follow a regression framework by minimizing\nthe squared loss between y\u02c6ui and its target value yui.\nTo handle the absence of negative data, they have either\ntreated all unobserved entries as negative feedback, or sampled\nnegative instances from unobserved entries [14]. For\npairwise learning [27, 44], the idea is that observed entries\nshould be ranked higher than the unobserved ones. As such,\ninstead of minimizing the loss between y\u02c6ui and yui, pairwise\nlearning maximizes the margin between observed entry y\u02c6ui\nand unobserved entry y\u02c6uj .\nMoving one step forward, our NCF framework parameterizes\nthe interaction function f using neural networks to\nestimate y\u02c6ui. As such, it naturally supports both pointwise\nand pairwise learning.\n2.2\nMatrix Factorization\nMF associates each user and item with a real-valued vector\nof latent features. Let pu and qi denote the latent vector for\nuser u and item i, respectively; MF estimates an interaction\nyui as the inner product of pu and qi:\ny\u02c6ui = f (u, i|pu, qi) = pTu qi =\npukqik,\n(2)\nK\nk=1\nwhere K denotes the dimension of the latent space. As we\ncan see, MF models the two-way interaction of user and item\nlatent factors, assuming each dimension of the latent space\nis independent of each other and linearly combining them\nwith the same weight. As such, MF can be deemed as a\nlinear model of latent factors.\nFigure 1 illustrates how the inner product function can\nlimit the expressiveness of MF. There are two settings to be\nstated clearly beforehand to understand the example well.\nFirst, since MF maps users and items to the same latent\nspace, the similarity between two users can also be measured\nwith an inner product, or equivalently2, the cosine of the\nangle between their latent vectors. Second, without loss of\n2Assuming latent vectors are of a unit length.</zone>\n  <zone label=\"MET_BIB_INFO\">Output Layer</zone>\n  <zone label=\"BODY_CONTENT\">Neural CF Layers</zone>\n  <zone label=\"MET_BIB_INFO\">Score \u0177ui</zone>\n  <zone label=\"MET_ABSTRACT\">Training yui Target</zone>\n  <zone label=\"BODY_CONTENT\">Layer X\n\u2026\u2026\nLayer 2\nLayer 1\nEmbedding Layer User Latent Vector\nPM\u00d7K = {puk}\nInput Layer (Sparse) 0 0 0 1 0 0 \u2026\u2026\nUser (u)\nItem Latent Vector\nQN\u00d7K = {qik}\n0 0 0 0 1 0 \u2026\u2026\nItem (i)\nFigure 2: Neural collaborative filtering framework\ngenerality, we use the Jaccard coefficient3 as the groundtruth\nsimilarity of two users that MF needs to recover.\nLet us first focus on the first three rows (users) in Figure\n1a. It is easy to have s23(0.66) &gt; s12(0.5) &gt; s13(0.4).\nAs such, the geometric relations of p1, p2, and p3 in the latent\nspace can be plotted as in Figure 1b. Now, let us consider\na new user u4, whose input is given as the dashed line\nin Figure 1a. We can have s41(0.6) &gt; s43(0.4) &gt; s42(0.2),\nmeaning that u4 is most similar to u1, followed by u3, and\nlastly u2. However, if a MF model places p4 closest to p1\n(the two options are shown in Figure 1b with dashed lines),\nit will result in p4 closer to p2 than p3, which unfortunately\nwill incur a large ranking loss.\nThe above example shows the possible limitation of MF\ncaused by the use of a simple and fixed inner product to estimate\ncomplex user-item interactions in the low-dimensional\nlatent space. We note that one way to resolve the issue is\nto use a large number of latent factors K. However, it may\nadversely hurt the generalization of the model (e.g., overfitting\nthe data), especially in sparse settings [26]. In this\nwork, we address the limitation by learning the interaction\nfunction using DNNs from data.</zone>\n  <zone label=\"GEN_OTHER\">3.</zone>\n  <zone label=\"BODY_HEADING\">NEURAL COLLABORATIVE FILTERING</zone>\n  <zone label=\"BODY_CONTENT\">We first present the general NCF framework, elaborating\nhow to learn NCF with a probabilistic model that emphasizes\nthe binary property of implicit data. We then\nshow that MF can be expressed and generalized under NCF.\nTo explore DNNs for collaborative filtering, we then propose\nan instantiation of NCF, using a multi-layer perceptron\n(MLP) to learn the user-item interaction function. Lastly,\nwe present a new neural matrix factorization model, which\nensembles MF and MLP under the NCF framework; it unifies\nthe strengths of linearity of MF and non-linearity of\nMLP for modelling the user-item latent structures.\n3.1</zone>\n  <zone label=\"BODY_HEADING\">General Framework</zone>\n  <zone label=\"BODY_CONTENT\">To permit a full neural treatment of collaborative filtering,\nwe adopt a multi-layer representation to model a user-item\ninteraction yui as shown in Figure 2, where the output of one\nlayer serves as the input of the next one. The bottom input\nlayer consists of two feature vectors vuU and viI that describe\nuser u and item i, respectively; they can be customized to\nsupport a wide range of modelling of users and items, such\n3Let Ru be the set of items that user u has interacted with,\nthen the Jaccard similarity of users i and j is defined as\nsij = |Ri|\u2229|Rj| .\n|Ri|\u222a|Rj|</zone>\n  <zone label=\"GEN_OTHER\">175</zone>\n  <zone label=\"BODY_CONTENT\">as context-aware [28, 1], content-based [3], and neighborbased\n[26]. Since this work focuses on the pure collaborative\nfiltering setting, we use only the identity of a user and an\nitem as the input feature, transforming it to a binarized\nsparse vector with one-hot encoding. Note that with such a\ngeneric feature representation for inputs, our method can be\neasily adjusted to address the cold-start problem by using\ncontent features to represent users and items.\nAbove the input layer is the embedding layer; it is a fully\nconnected layer that projects the sparse representation to\na dense vector. The obtained user (item) embedding can\nbe seen as the latent vector for user (item) in the context\nof latent factor model. The user embedding and item embedding\nare then fed into a multi-layer neural architecture,\nwhich we term as neural collaborative filtering layers, to map\nthe latent vectors to prediction scores. Each layer of the neural\nCF layers can be customized to discover certain latent\nstructures of user-item interactions. The dimension of the\nlast hidden layer X determines the model's capability. The\nfinal output layer is the predicted score y\u02c6ui, and training\nis performed by minimizing the pointwise loss between y\u02c6ui\nand its target value yui. We note that another way to train\nthe model is by performing pairwise learning, such as using\nthe Bayesian Personalized Ranking [27] and margin-based\nloss [33]. As the focus of the paper is on the neural network\nmodelling part, we leave the extension to pairwise learning\nof NCF as a future work.\nWe now formulate the NCF's predictive model as\ny\u02c6ui = f (PT vuU , QT viI |P, Q, \u0398f ),\n(3)\nwhere P \u2208 RM\u00d7K and Q \u2208 RN\u00d7K , denoting the latent factor\nmatrix for users and items, respectively; and \u0398f denotes\nthe model parameters of the interaction function f . Since\nthe function f is defined as a multi-layer neural network, it\ncan be formulated as\nf (PT vuU , QT viI ) = \u03c6out(\u03c6X (...\u03c62(\u03c61(PT vuU , QT viI ))...)),\n(4)\nwhere \u03c6out and \u03c6x respectively denote the mapping function\nfor the output layer and x-th neural collaborative filtering\n(CF) layer, and there are X neural CF layers in total.\n3.1.1</zone>\n  <zone label=\"BODY_HEADING\">Learning NCF</zone>\n  <zone label=\"BODY_CONTENT\">To learn model parameters, existing pointwise methods [14,\n39] largely perform a regression with squared loss:\nLsqr =\nwui(yui \u2212 y\u02c6ui)2,\n(5)\n(u,i)\u2208Y\u222aYwhere\nY denotes the set of observed interactions in Y, and\nY\u2212 denotes the set of negative instances, which can be all (or\nsampled from) unobserved interactions; and wui is a hyperparameter\ndenoting the weight of training instance (u, i).\nWhile the squared loss can be explained by assuming that\nobservations are generated from a Gaussian distribution [29],\nwe point out that it may not tally well with implicit data.\nThis is because for implicit data, the target value yui is\na binarized 1 or 0 denoting whether u has interacted with\ni. In what follows, we present a probabilistic approach for\nlearning the pointwise NCF that pays special attention to\nthe binary property of implicit data.\nConsidering the one-class nature of implicit feedback, we\ncan view the value of yui as a label - 1 means item i is\nrelevant to u, and 0 otherwise. The prediction score y\u02c6ui\nthen represents how likely i is relevant to u. To endow NCF\nwith such a probabilistic explanation, we need to constrain\nthe output y\u02c6ui in the range of [0, 1], which can be easily\nachieved by using a probabilistic function (e.g., the Logistic\nor Probit function) as the activation function for the output\nlayer \u03c6out. With the above settings, we then define the\nlikelihood function as\np(Y, Y\u2212|P, Q, \u0398f ) =\ny\u02c6ui\n(1 \u2212 y\u02c6uj ).\n(6)\n(u,i)\u2208Y\n(u,j)\u2208YTaking\nthe negative logarithm of the likelihood, we reach\nlog y\u02c6ui log(1\n\u2212 y\u02c6uj )\n(u,j)\u2208Yyui\nlog y\u02c6ui + (1 \u2212 yui) log(1 \u2212 y\u02c6ui).\nL = =\n(u,i)\u2208Y\n(u,i)\u2208Y\u222aYThis\nis the objective function to minimize for the NCF methods,\nand its optimization can be done by performing stochastic\ngradient descent (SGD). Careful readers might have realized\nthat it is the same as the binary cross-entropy loss, also\nknown as log loss. By employing a probabilistic treatment\nfor NCF, we address recommendation with implicit feedback\nas a binary classification problem. As the classificationaware\nlog loss has rarely been investigated in recommendation\nliterature, we explore it in this work and empirically\nshow its effectiveness in Section 4.3. For the negative instances\nY\u2212, we uniformly sample them from unobserved interactions\nin each iteration and control the sampling ratio\nw.r.t. the number of observed interactions. While a nonuniform\nsampling strategy (e.g., item popularity-biased [14,\n12]) might further improve the performance, we leave the\nexploration as a future work.\n3.2</zone>\n  <zone label=\"BODY_HEADING\">Generalized Matrix Factorization (GMF)</zone>\n  <zone label=\"BODY_CONTENT\">We now show how MF can be interpreted as a special case\nof our NCF framework. As MF is the most popular model\nfor recommendation and has been investigated extensively\nin literature, being able to recover it allows NCF to mimic\na large family of factorization models [26].\nDue to the one-hot encoding of user (item) ID of the input\nlayer, the obtained embedding vector can be seen as the\nlatent vector of user (item). Let the user latent vector pu\nbe PT vuU and item latent vector qi be QT viI . We define the\nmapping function of the first neural CF layer as\n\u03c61(pu, qi) = pu\nqi,\nwhere denotes the element-wise product of vectors. We\nthen project the vector to the output layer:\ny\u02c6ui = aout(hT (pu\nqi)),\nwhere aout and h denote the activation function and edge\nweights of the output layer, respectively. Intuitively, if we\nuse an identity function for aout and enforce h to be a uniform\nvector of 1, we can exactly recover the MF model.\nUnder the NCF framework, MF can be easily generalized\nand extended. For example, if we allow h to be learnt\nfrom data without the uniform constraint, it will result in\na variant of MF that allows varying importance of latent\ndimensions. And if we use a non-linear function for aout, it\nwill generalize MF to a non-linear setting which might be\nmore expressive than the linear MF model. In this work, we\nimplement a generalized version of MF under NCF that uses\n(7)\n(8)\n(9)</zone>\n  <zone label=\"GEN_OTHER\">176</zone>\n  <zone label=\"BODY_CONTENT\">the sigmoid function \u03c3(x) = 1/(1 + e\u2212x) as aout and learns\nh from data with the log loss (Section 3.1.1). We term it as\nGMF, short for Generalized Matrix Factorization.\n3.3</zone>\n  <zone label=\"BODY_HEADING\">Multi-Layer Perceptron (MLP)</zone>\n  <zone label=\"BODY_CONTENT\">Since NCF adopts two pathways to model users and items,\nit is intuitive to combine the features of two pathways by\nconcatenating them. This design has been widely adopted\nin multimodal deep learning work [47, 34]. However, simply\na vector concatenation does not account for any interactions\nbetween user and item latent features, which is insufficient\nfor modelling the collaborative filtering effect. To address\nthis issue, we propose to add hidden layers on the concatenated\nvector, using a standard MLP to learn the interaction\nbetween user and item latent features. In this sense, we can\nendow the model a large level of flexibility and non-linearity\nto learn the interactions between pu and qi, rather than the\nway of GMF that uses only a fixed element-wise product\non them. More precisely, the MLP model under our NCF\nframework is defined as\nz1 = \u03c61(pu, qi) =\n\u03c62(z1) = a2(W2T z1 + b2),\npu ,\nqi\n......\n\u03c6L(zL\u22121) = aL(WTLzL\u22121 + bL),\ny\u02c6ui = \u03c3(hT \u03c6L(zL\u22121)),\n(10)\nwhere Wx, bx, and ax denote the weight matrix, bias vector,\nand activation function for the x-th layer's perceptron,\nrespectively. For activation functions of MLP layers, one\ncan freely choose sigmoid, hyperbolic tangent (tanh), and\nRectifier (ReLU), among others. We would like to analyze\neach function: 1) The sigmoid function restricts each\nneuron to be in (0,1), which may limit the model's performance;\nand it is known to suffer from saturation, where\nneurons stop learning when their output is near either 0 or\n1. 2) Even though tanh is a better choice and has been\nwidely adopted [6, 44], it only alleviates the issues of sigmoid\nto a certain extent, since it can be seen as a rescaled\nversion of sigmoid (tanh(x/2) = 2\u03c3(x) \u2212 1). And 3) as\nsuch, we opt for ReLU, which is more biologically plausible\nand proven to be non-saturated [9]; moreover, it encourages\nsparse activations, being well-suited for sparse data and\nmaking the model less likely to be overfitting. Our empirical\nresults show that ReLU yields slightly better performance\nthan tanh, which in turn is significantly better than sigmoid.\nAs for the design of network structure, a common solution\nis to follow a tower pattern, where the bottom layer is the\nwidest and each successive layer has a smaller number of\nneurons (as in Figure 2). The premise is that by using a\nsmall number of hidden units for higher layers, they can\nlearn more abstractive features of data [10]. We empirically\nimplement the tower structure, halving the layer size for\neach successive higher layer.\n3.4</zone>\n  <zone label=\"BODY_HEADING\">Fusion of GMF and MLP</zone>\n  <zone label=\"BODY_CONTENT\">So far we have developed two instantiations of NCF GMF\nthat applies a linear kernel to model the latent feature\ninteractions, and MLP that uses a non-linear kernel to learn\nthe interaction function from data. The question then arises:\nhow can we fuse GMF and MLP under the NCF framework,\nGMF Layer\nElement-wise\nProduct\nScore \u0177ui\n\u08cc\nNeuMF Layer\nConcatenation</zone>\n  <zone label=\"MET_BIB_INFO\">Training\nLog loss</zone>\n  <zone label=\"MET_ABSTRACT\">yui Target</zone>\n  <zone label=\"BODY_CONTENT\">MLP Layer X\n\u2026\u2026 ReLU\nMLP Layer 2\nReLU\nMLP Layer 1\nConcatenation\nMF User Vector\nMLP User Vector\nMF Item Vector\nMLP Item Vector\n0 0 0 1 0 0 \u2026\u2026\nUser (u)\n0 0 0 0 1 0 \u2026\u2026\nItem ( i )\nFigure 3: Neural matrix factorization model\nso that they can mutually reinforce each other to better\nmodel the complex user-iterm interactions?\nA straightforward solution is to let GMF and MLP share\nthe same embedding layer, and then combine the outputs of\ntheir interaction functions. This way shares a similar spirit\nwith the well-known Neural Tensor Network (NTN) [33].\nSpecifically, the model for combining GMF with a one-layer\nMLP can be formulated as\ny\u02c6ui = \u03c3(hT a(pu\nqi + W\npu + b)).\nqi\n(11)\nHowever, sharing embeddings of GMF and MLP might\nlimit the performance of the fused model. For example,\nit implies that GMF and MLP must use the same size of\nembeddings; for datasets where the optimal embedding size\nof the two models varies a lot, this solution may fail to obtain\nthe optimal ensemble.\nTo provide more flexibility to the fused model, we allow\nGMF and MLP to learn separate embeddings, and combine\nthe two models by concatenating their last hidden layer.\nFigure 3 illustrates our proposal, the formulation of which\nis given as follows\n+ b2)...)) + bL),\n\u03c6GMF = pu\nG\nqiG,\n\u03c6MLP = aL(WTL(aL\u22121(...a2(W2T puM\nqiM\ny\u02c6ui = \u03c3(hT \u03c6GMF\n\u03c6MLP ),\n(12)\nwhere puG and puM denote the user embedding for GMF\nand MLP parts, respectively; and similar notations of qiG\nand qiM for item embeddings. As discussed before, we use\nReLU as the activation function of MLP layers. This model\ncombines the linearity of MF and non-linearity of DNNs for\nmodelling user-item latent structures. We dub this model\n\u201cNeuMF\u201d, short for Neural Matrix Factorization. The derivative\nof the model w.r.t. each model parameter can be calculated\nwith standard back-propagation, which is omitted\nhere due to space limitation.\n3.4.1</zone>\n  <zone label=\"BODY_HEADING\">Pre-training</zone>\n  <zone label=\"BODY_CONTENT\">Due to the non-convexity of the objective function of NeuMF,\ngradient-based optimization methods only find locally-optimal\nsolutions. It is reported that the initialization plays an important\nrole for the convergence and performance of deep\nlearning models [7]. Since NeuMF is an ensemble of GMF\nand MLP, we propose to initialize NeuMF using the pretrained\nmodels of GMF and MLP.\nWe first train GMF and MLP with random initializations\nuntil convergence. We then use their model parameters as\nthe initialization for the corresponding parts of NeuMF's\nparameters. The only tweak is on the output layer, where\nwe concatenate weights of the two models with\nh \u2190\n\u03b1hGMF\n(1 \u2212 \u03b1)hMLP ,\n(13)\nwhere hGMF and hMLP denote the h vector of the pretrained\nGMF and MLP model, respectively; and \u03b1 is a\nhyper-parameter determining the trade-off between the two\npre-trained models.\nFor training GMF and MLP from scratch, we adopt the\nAdaptive Moment Estimation (Adam) [20], which adapts\nthe learning rate for each parameter by performing smaller\nupdates for frequent and larger updates for infrequent parameters.\nThe Adam method yields faster convergence for\nboth models than the vanilla SGD and relieves the pain of\ntuning the learning rate. After feeding pre-trained parameters\ninto NeuMF, we optimize it with the vanilla SGD, rather\nthan Adam. This is because Adam needs to save momentum\ninformation for updating parameters properly. As we initialize\nNeuMF with pre-trained model parameters only and\nforgo saving the momentum information, it is unsuitable to\nfurther optimize NeuMF with momentum-based methods.\n4.</zone>\n  <zone label=\"BODY_HEADING\">EXPERIMENTS</zone>\n  <zone label=\"BODY_CONTENT\">In this section, we conduct experiments with the aim of\nanswering the following research questions:\nRQ1 Do our proposed NCF methods outperform the stateof-the-art\nimplicit collaborative filtering methods?\nRQ2 How does our proposed optimization framework (log\nloss with negative sampling) work for the recommendation\ntask?\nRQ3 Are deeper layers of hidden units helpful for learning\nfrom user-item interaction data?\nIn what follows, we first present the experimental settings,\nfollowed by answering the above three research questions.\n4.1</zone>\n  <zone label=\"BODY_HEADING\">Experimental Settings</zone>\n  <zone label=\"BODY_CONTENT\">Datasets. We experimented with two publicly accessible\ndatasets: MovieLens4 and Pinterest5. The characteristics of\nthe two datasets are summarized in Table 1.\n1. MovieLens. This movie rating dataset has been\nwidely used to evaluate collaborative filtering algorithms.\nWe used the version containing one million ratings, where\neach user has at least 20 ratings. While it is an explicit\nfeedback data, we have intentionally chosen it to investigate\nthe performance of learning from the implicit signal [21] of\nexplicit feedback. To this end, we transformed it into implicit\ndata, where each entry is marked as 0 or 1 indicating\nwhether the user has rated the item.\n2. Pinterest. This implicit feedback data is constructed\nby [8] for evaluating content-based image recommendation.\n4http://grouplens.org/datasets/movielens/1m/\n5https://sites.google.com/site/xueatalphabeta/\nacademic-projects</zone>\n  <zone label=\"GEN_OTHER\">177</zone>\n  <zone label=\"BODY_CONTENT\">Table 1: Statistics of the evaluation datasets.\nDataset\nMovieLens\nPinterest\nInteraction#\n1,000,209\n1,500,809\nItem#\n3,706\n9,916\nUser#\n6,040\n55,187\nSparsity\n95.53%\n99.73%\nThe original data is very large but highly sparse. For example,\nover 20% of users have only one pin, making it difficult\nto evaluate collaborative filtering algorithms. As such, we\nfiltered the dataset in the same way as the MovieLens data\nthat retained only users with at least 20 interactions (pins).\nThis results in a subset of the data that contains 55, 187\nusers and 1, 500, 809 interactions. Each interaction denotes\nwhether the user has pinned the image to her own board.\nEvaluation Protocols. To evaluate the performance of\nitem recommendation, we adopted the leave-one-out evaluation,\nwhich has been widely used in literature [1, 14, 27].\nFor each user, we held-out her latest interaction as the test\nset and utilized the remaining data for training. Since it is\ntoo time-consuming to rank all items for every user during\nevaluation, we followed the common strategy [6, 21] that\nrandomly samples 100 items that are not interacted by the\nuser, ranking the test item among the 100 items. The performance\nof a ranked list is judged by Hit Ratio (HR) and Normalized\nDiscounted Cumulative Gain (NDCG) [11]. Without\nspecial mention, we truncated the ranked list at 10 for\nboth metrics. As such, the HR intuitively measures whether\nthe test item is present on the top-10 list, and the NDCG\naccounts for the position of the hit by assigning higher scores\nto hits at top ranks. We calculated both metrics for each\ntest user and reported the average score.\nBaselines. We compared our proposed NCF methods (GMF,\nMLP and NeuMF) with the following methods:\n- ItemPop. Items are ranked by their popularity judged\nby the number of interactions. This is a non-personalized\nmethod to benchmark the recommendation performance [27].\n- ItemKNN [31]. This is the standard item-based collaborative\nfiltering method. We followed the setting of [19]\nto adapt it for implicit data.\n- BPR [27]. This method optimizes the MF model of\nEquation 2 with a pairwise ranking loss, which is tailored\nto learn from implicit feedback. It is a highly competitive\nbaseline for item recommendation. We used a fixed learning\nrate, varying it and reporting the best performance.\n- eALS [14]. This is a state-of-the-art MF method for\nitem recommendation. It optimizes the squared loss of Equation\n5, treating all unobserved interactions as negative instances\nand weighting them non-uniformly by the item popularity.\nSince eALS shows superior performance over the\nuniform-weighting method WMF [19], we do not further report\nWMF's performance.\nAs our proposed methods aim to model the relationship\nbetween users and items, we mainly compare with useritem\nmodels. We leave out the comparison with item-item\nmodels, such as SLIM [25] and CDAE [44], because the performance\ndifference may be caused by the user models for\npersonalization (as they are item-item model).\nParameter Settings. We implemented our proposed methods\nbased on Keras6. To determine hyper-parameters of\nNCF methods, we randomly sampled one interaction for\n6https://github.com/hexiangnan/neural_\ncollaborative_filtering</zone>\n  <zone label=\"GEN_OTHER\">178</zone>\n  <zone label=\"BODY_CONTENT\">each user as the validation data and tuned hyper-parameters\non it. All NCF models are learnt by optimizing the log loss\nof Equation 7, where we sampled four negative instances\nper positive instance. For NCF models that are trained\nfrom scratch, we randomly initialized model parameters with\na Gaussian distribution (with a mean of 0 and standard\ndeviation of 0.01), optimizing the model with mini-batch\nAdam [20]. We tested the batch size of [128, 256, 512, 1024],\nand the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since\nthe last hidden layer of NCF determines the model capability,\nwe term it as predictive factors and evaluated the\nfactors of [8, 16, 32, 64]. It is worth noting that large factors\nmay cause overfitting and degrade the performance. Without\nspecial mention, we employed three hidden layers for\nMLP; for example, if the size of predictive factors is 8, then\nthe architecture of the neural CF layers is 32 \u2192 16 \u2192 8, and\nthe embedding size is 16. For the NeuMF with pre-training,\n\u03b1 was set to 0.5, allowing the pre-trained GMF and MLP to\ncontribute equally to NeuMF's initialization.\n4.2</zone>\n  <zone label=\"BODY_HEADING\">Performance Comparison (RQ1)</zone>\n  <zone label=\"BODY_CONTENT\">Figure 4 shows the performance of HR@10 and NDCG@10\nwith respect to the number of predictive factors. For MF\nmethods BPR and eALS, the number of predictive factors\nis equal to the number of latent factors. For ItemKNN, we\ntested different neighbor sizes and reported the best performance.\nDue to the weak performance of ItemPop, it is\nomitted in Figure 4 to better highlight the performance difference\nof personalized methods.\nFirst, we can see that NeuMF achieves the best performance\non both datasets, significantly outperforming the stateof-the-art\nmethods eALS and BPR by a large margin (on\naverage, the relative improvement over eALS and BPR is\n4.5% and 4.9%, respectively). For Pinterest, even with a\nsmall predictive factor of 8, NeuMF substantially outperforms\nthat of eALS and BPR with a large factor of 64. This\nindicates the high expressiveness of NeuMF by fusing the\nlinear MF and non-linear MLP models. Second, the other\ntwo NCF methods - GMF and MLP - also show quite\nstrong performance. Between them, MLP slightly underperforms\nGMF. Note that MLP can be further improved by\nadding more hidden layers (see Section 4.4), and here we\nonly show the performance of three layers. For small predictive\nfactors, GMF outperforms eALS on both datasets;\nalthough GMF suffers from overfitting for large factors, its\nbest performance obtained is better than (or on par with)\nthat of eALS. Lastly, GMF shows consistent improvements\nover BPR, admitting the effectiveness of the classificationaware\nlog loss for the recommendation task, since GMF and\nBPR learn the same MF model but with different objective\nfunctions.\nFigure 5 shows the performance of Top-K recommended\nlists where the ranking position K ranges from 1 to 10. To\nmake the figure more clear, we show the performance of\nNeuMF rather than all three NCF methods. As can be\nseen, NeuMF demonstrates consistent improvements over\nother methods across positions, and we further conducted\none-sample paired t-tests, verifying that all improvements\nare statistically significant for p &lt; 0.01. For baseline methods,\neALS outperforms BPR on MovieLens with about 5.1%\nrelative improvement, while underperforms BPR on Pinterest\nin terms of NDCG. This is consistent with [14]'s finding\nthat BPR can be a strong performer for ranking performance\n$ %\n#\n!!\n\"#\n&amp;\n!\n$ %\n#\n!!\n\"#\n&amp;\n!\n$ %\n#\n!!\n\"#\n&amp;\n!\n$ %\n#\n!!\n\"#\n&amp;\n!</zone>\n  <zone label=\"MET_CORRESPONDENCE\">(a) MovieLens - HR@10 (b) MovieLens - NDCG@10\n(c) Pinterest - HR@10\n(d) Pinterest - NDCG@10</zone>\n  <zone label=\"BODY_CONTENT\">Figure 4: Performance of HR@10 and NDCG@10 w.r.t. the number of predictive factors on the two datasets.</zone>\n  <zone label=\"GEN_OTHER\">(a) MovieLens - HR@K\n(b) MovieLens - NDCG@K\n(c) Pinterest - HR@K\n(d) Pinterest - NDCG@K</zone>\n  <zone label=\"BODY_CONTENT\">Figure 5: Evaluation of Top-K item recommendation where K ranges from 1 to 10 on the two datasets.\n&amp;\n*\n'\n!\n&amp;\n*\n'\n!\n\"#\n!\n# ,\n$ %\n!!\n+\n\"#\n!\n# ,\n$ %\n!!\n+\nowing to its pairwise ranking-aware learner. The neighborbased\nItemKNN underperforms model-based methods. And\nItemPop performs the worst, indicating the necessity of modeling\nusers' personalized preferences, rather than just recommending\npopular items to users.\n4.2.1</zone>\n  <zone label=\"BODY_HEADING\">Utility of Pre-training</zone>\n  <zone label=\"BODY_CONTENT\">To demonstrate the utility of pre-training for NeuMF, we\ncompared the performance of two versions of NeuMF with\nand without pre-training. For NeuMF without pretraining,\nwe used the Adam to learn it with random initializations.\nAs shown in Table 2, the NeuMF with pretraining\nachieves better performance in most cases; only\nfor MovieLens with a small predictive factors of 8, the pretraining\nmethod performs slightly worse. The relative improvements\nof the NeuMF with pre-training are 2.2% and\n1.1% for MovieLens and Pinterest, respectively. This result\njustifies the usefulness of our pre-training method for\ninitializing NeuMF.\nTable 2: Performance of NeuMF with and without\npre-training.\nFactors</zone>\n  <zone label=\"MET_CORRESPONDENCE\">With Pre-training\nHR@10 NDCG@10\nWithout Pre-training\nHR@10 NDCG@10</zone>\n  <zone label=\"BODY_CONTENT\">8\n16\n32\n64\n8\n16\n32\n64\n0.684\n0.707\n0.726\n0.730\n0.878\n0.880\n0.879\n0.877\nMovieLens\n0.403\n0.426\n0.445\n0.447\nPinterest\n0.555\n0.558\n0.555\n0.552\n0.688\n0.696\n0.701\n0.705\n0.869\n0.871\n0.870\n0.872\n0.410\n0.420\n0.425\n0.426\n0.546\n0.547\n0.549\n0.551\n\"#\n$ %\n!\n# ,\n!!\n+\n&amp;\n*\n'\n!\n&amp;\n*\n'\n!\n#\n#\n\"#\n$ %\n!\n# ,\n!!\n+\n+\n+\n#\n#</zone>\n  <zone label=\"GEN_OTHER\">179</zone>\n  <zone label=\"BODY_CONTENT\">4.3</zone>\n  <zone label=\"BODY_HEADING\">Log Loss with Negative Sampling (RQ2)</zone>\n  <zone label=\"BODY_CONTENT\">To deal with the one-class nature of implicit feedback,\nwe cast recommendation as a binary classification task. By\nviewing NCF as a probabilistic model, we optimized it with\nthe log loss. Figure 6 shows the training loss (averaged\nover all instances) and recommendation performance of NCF\nmethods of each iteration on MovieLens. Results on Pinterest\nshow the same trend and thus they are omitted due to\nspace limitation. First, we can see that with more iterations,\nthe training loss of NCF models gradually decreases and\nthe recommendation performance is improved. The most\neffective updates are occurred in the first 10 iterations, and\nmore iterations may overfit a model (e.g., although the training\nloss of NeuMF keeps decreasing after 10 iterations, its\nrecommendation performance actually degrades). Second,\namong the three NCF methods, NeuMF achieves the lowest\ntraining loss, followed by MLP, and then GMF. The recommendation\nperformance also shows the same trend that\nNeuMF &gt; MLP &gt; GMF. The above findings provide empirical\nevidence for the rationality and effectiveness of optimizing\nthe log loss for learning from implicit data.\nAn advantage of pointwise log loss over pairwise objective\nfunctions [27, 33] is the flexible sampling ratio for negative\ninstances. While pairwise objective functions can pair only\none sampled negative instance with a positive instance, we\ncan flexibly control the sampling ratio of a pointwise loss. To\nillustrate the impact of negative sampling for NCF methods,\nwe show the performance of NCF methods w.r.t. different\nnegative sampling ratios in Figure 7. It can be clearly seen\nthat just one negative sample per positive instance is insufficient\nto achieve optimal performance, and sampling more\nnegative instances is beneficial. Comparing GMF to BPR,\nwe can see the performance of GMF with a sampling ratio\nof one is on par with BPR, while GMF significantly betters\n/\n.\n&amp;\n!\n#\n&amp;\n!\n#\n&amp;\n*\n'\n!\n+\n#\n#\n&amp;\n!\n#\n(a) Training Loss</zone>\n  <zone label=\"MET_CORRESPONDENCE\">(b) HR@10\n(c) NDCG@10</zone>\n  <zone label=\"BODY_CONTENT\">Figure 6: Training loss and recommendation performance of NCF methods w.r.t. the number of iterations\non MovieLens (factors=8).\n!\n&amp;\n\"#\n#\n+\n&amp;\n*\n'\n!\n!\n&amp;\n\"#\n#\n+\n!\n&amp;\n\"#\n#\n+\n&amp;\n*\n'\n!\n!\n#\n&amp;\n\"#\n+\n! 0 / 3/! . ! 0 / 3/! .\n(a) MovieLens - HR@10 (b) MovieLens - NDCG@10\n! 0 / 3/! .\n(c) Pinterest - HR@10\n! 0 / 3/! .\n(d) Pinterest - NDCG@10\nFigure 7: Performance of NCF methods w.r.t. the number of negative samples per positive instance (factors=16).\nThe performance of BPR is also shown, which samples only one negative instance to pair with a\npositive instance for learning.\nBPR with larger sampling ratios. This shows the advantage\nof pointwise log loss over the pairwise BPR loss. For\nboth datasets, the optimal sampling ratio is around 3 to 6.\nOn Pinterest, we find that when the sampling ratio is larger\nthan 7, the performance of NCF methods starts to drop. It\nreveals that setting the sampling ratio too aggressively may\nadversely hurt the performance.\n4.4</zone>\n  <zone label=\"BODY_HEADING\">Is Deep Learning Helpful? (RQ3)</zone>\n  <zone label=\"BODY_CONTENT\">As there is little work on learning user-item interaction\nfunction with neural networks, it is curious to see whether\nusing a deep network structure is beneficial to the recommendation\ntask. Towards this end, we further investigated\nMLP with different number of hidden layers. The results\nare summarized in Table 3 and 4. The MLP-3 indicates\nthe MLP method with three hidden layers (besides the embedding\nlayer), and similar notations for others. As we can\nsee, even for models with the same capability, stacking more\nlayers are beneficial to performance. This result is highly\nencouraging, indicating the effectiveness of using deep models\nfor collaborative recommendation. We attribute the improvement\nto the high non-linearities brought by stacking\nmore non-linear layers. To verify this, we further tried stacking\nlinear layers, using an identity function as the activation\nfunction. The performance is much worse than using the\nReLU unit.\nFor MLP-0 that has no hidden layers (i.e., the embedding\nlayer is directly projected to predictions), the performance is\nvery weak and is not better than the non-personalized ItemPop.\nThis verifies our argument in Section 3.3 that simply\nconcatenating user and item latent vectors is insufficient for\nmodelling their feature interactions, and thus the necessity\nof transforming it with hidden layers.</zone>\n  <zone label=\"GEN_OTHER\">180\nTable 3: HR@10 of MLP with different layers.</zone>\n  <zone label=\"BODY_CONTENT\">Factors\nMLP-0\nMLP-1\nMLP-2\nMLP-3\nMLP-4\n8\n16\n32\n64\n8\n16\n32\n64\n0.452\n0.454\n0.453\n0.453\n0.275\n0.274\n0.273\n0.274\nMovieLens\n0.628 0.655\n0.663 0.674\n0.682 0.687\n0.687 0.696\nPinterest\n0.848 0.855\n0.855 0.861\n0.861 0.863\n0.864 0.867\n0.671\n0.684\n0.692\n0.702\n0.859\n0.865\n0.868\n0.869\n0.678\n0.690\n0.699\n0.707\n0.862\n0.867\n0.867\n0.873\n5.\nRELATED WORK\nWhile early literature on recommendation has largely focused\non explicit feedback [30, 31], recent attention is increasingly\nshifting towards implicit data [1, 14, 23]. The\ncollaborative filtering (CF) task with implicit feedback is\nusually formulated as an item recommendation problem, for\nwhich the aim is to recommend a short list of items to users.\nIn contrast to rating prediction that has been widely solved\nby work on explicit feedback, addressing the item recommendation\nproblem is more practical but challenging [1, 11]. One\nkey insight is to model the missing data, which are always\nignored by the work on explicit feedback [21, 48]. To tailor\nlatent factor models for item recommendation with implicit\nfeedback, early work [19, 27] applies a uniform weighting\nwhere two strategies have been proposed - which either\ntreated all missing data as negative instances [19] or sampled\nnegative instances from missing data [27]. Recently, He\net al. [14] and Liang et al. [23] proposed dedicated models\nto weight missing data, and Rendle et al. [1] developed an\nTable 4: NDCG@10 of MLP with different layers.\nFactors\nMLP-0\nMLP-1\nMLP-2\nMLP-3\nMLP-4\n8\n16\n32\n64\n8\n16\n32\n64\n0.253\n0.252\n0.252\n0.251\n0.141\n0.141\n0.142\n0.141\nMovieLens\n0.359 0.383\n0.391 0.402\n0.406 0.410\n0.409 0.417\nPinterest\n0.526 0.534\n0.532 0.536\n0.537 0.538\n0.538 0.542\n0.399\n0.410\n0.425\n0.426\n0.536\n0.538\n0.542\n0.545\n0.406\n0.415\n0.423\n0.432\n0.539\n0.544\n0.546\n0.550\nimplicit coordinate descent (iCD) solution for feature-based\nfactorization models, achieving state-of-the-art performance\nfor item recommendation. In the following, we discuss recommendation\nworks that use neural networks.\nThe early pioneer work by Salakhutdinov et al. [30] proposed\na two-layer Restricted Boltzmann Machines (RBMs)\nto model users' explicit ratings on items. The work was been\nlater extended to model the ordinal nature of ratings [36].\nRecently, autoencoders have become a popular choice for\nbuilding recommendation systems [32, 22, 35]. The idea of\nuser-based AutoRec [32] is to learn hidden structures that\ncan reconstruct a user's ratings given her historical ratings\nas inputs. In terms of user personalization, this approach\nshares a similar spirit as the item-item model [31, 25] that\nrepresents a user as her rated items. To avoid autoencoders\nlearning an identity function and failing to generalize to unseen\ndata, denoising autoencoders (DAEs) have been applied\nto learn from intentionally corrupted inputs [22, 35]. More\nrecently, Zheng et al. [48] presented a neural autoregressive\nmethod for CF. While the previous effort has lent support\nto the effectiveness of neural networks for addressing CF,\nmost of them focused on explicit ratings and modelled the\nobserved data only. As a result, they can easily fail to learn\nusers' preference from the positive-only implicit data.\nAlthough some recent works [6, 37, 38, 43, 45] have explored\ndeep learning models for recommendation based on\nimplicit feedback, they primarily used DNNs for modelling\nauxiliary information, such as textual description of items [38],\nacoustic features of musics [37, 43], cross-domain behaviors\nof users [6], and the rich information in knowledge bases [45].\nThe features learnt by DNNs are then integrated with MF\nfor CF. The work that is most relevant to our work is [44],\nwhich presents a collaborative denoising autoencoder (CDAE)\nfor CF with implicit feedback. In contrast to the DAE-based\nCF [35], CDAE additionally plugs a user node to the input\nof autoencoders for reconstructing the user's ratings. As\nshown by the authors, CDAE is equivalent to the SVD++\nmodel [21] when the identity function is applied to activate\nthe hidden layers of CDAE. This implies that although\nCDAE is a neural modelling approach for CF, it still applies\na linear kernel (i.e., inner product) to model user-item interactions.\nThis may partially explain why using deep layers for\nCDAE does not improve the performance (cf. Section 6 of\n[44]). Distinct from CDAE, our NCF adopts a two-pathway\narchitecture, modelling user-item interactions with a multilayer\nfeedforward neural network. This allows NCF to learn\nan arbitrary function from the data, being more powerful\nand expressive than the fixed inner product function.\nAlong a similar line, learning the relations of two entities\nhas been intensively studied in literature of knowledge</zone>\n  <zone label=\"GEN_OTHER\">181</zone>\n  <zone label=\"BODY_CONTENT\">graphs [2, 33]. Many relational machine learning methods\nhave been devised [24]. The one that is most similar to our\nproposal is the Neural Tensor Network (NTN) [33], which\nuses neural networks to learn the interaction of two entities\nand shows strong performance. Here we focus on a different\nproblem setting of CF. While the idea of NeuMF that\ncombines MF with MLP is partially inspired by NTN, our\nNeuMF is more flexible and generic than NTN, in terms of\nallowing MF and MLP learning different sets of embeddings.\nMore recently, Google publicized their Wide &amp; Deep learning\napproach for App recommendation [4]. The deep component\nsimilarly uses a MLP on feature embeddings, which has\nbeen reported to have strong generalization ability. While\ntheir work has focused on incorporating various features\nof users and items, we target at exploring DNNs for pure\ncollaborative filtering systems. We show that DNNs are a\npromising choice for modelling user-item interactions, which\nto our knowledge has not been investigated before.</zone>\n  <zone label=\"GEN_REFERENCES\">6.</zone>\n  <zone label=\"BODY_HEADING\">CONCLUSION AND FUTURE WORK</zone>\n  <zone label=\"BODY_CONTENT\">In this work, we explored neural network architectures\nfor collaborative filtering. We devised a general framework\nNCF and proposed three instantiations - GMF, MLP and\nNeuMF - that model user-item interactions in different\nways. Our framework is simple and generic; it is not limited\nto the models presented in this paper, but is designed to\nserve as a guideline for developing deep learning methods for\nrecommendation. This work complements the mainstream\nshallow models for collaborative filtering, opening up a new\navenue of research possibilities for recommendation based\non deep learning.\nIn future, we will study pairwise learners for NCF models\nand extend NCF to model auxiliary information, such\nas user reviews [11], knowledge bases [45], and temporal signals\n[1]. While existing personalization models have primarily\nfocused on individuals, it is interesting to develop models\nfor groups of users, which help the decision-making for social\ngroups [15, 42]. Moreover, we are particularly interested in\nbuilding recommender systems for multi-media items, an interesting\ntask but has received relatively less scrutiny in the\nrecommendation community [3]. Multi-media items, such as\nimages and videos, contain much richer visual semantics [16,\n41] that can reflect users' interest. To build a multi-media\nrecommender system, we need to develop effective methods\nto learn from multi-view and multi-modal data [13, 40]. Another\nemerging direction is to explore the potential of recurrent\nneural networks and hashing methods [46] for providing\nefficient online recommendation [14, 1].\nAcknowledgement\nThe authors thank the anonymous reviewers for their valuable\ncomments, which are beneficial to the authors' thoughts\non recommendation systems and the revision of the paper.</zone>\n  <zone label=\"GEN_REFERENCES\">7. REFERENCES\n[1] I. Bayer, X. He, B. Kanagal, and S. Rendle. A generic\ncoordinate descent framework for learning from implicit\nfeedback. In WWW, 2017.\n[2] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and\nO. Yakhnenko. Translating embeddings for modeling\nmulti-relational data. In NIPS, pages 2787-2795, 2013.\n[3] T. Chen, X. He, and M.-Y. Kan. Context-aware image\ntweet modelling and recommendation. In MM, pages\n1018-1027, 2016.\n[4] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra,\nH. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir,\net al. Wide &amp; deep learning for recommender systems.\narXiv preprint arXiv:1606.07792, 2016.\n[5] R. Collobert and J. Weston. A unified architecture for\nnatural language processing: Deep neural networks with\nmultitask learning. In ICML, pages 160-167, 2008.\n[6] A. M. Elkahky, Y. Song, and X. He. A multi-view deep\nlearning approach for cross domain user modeling in\nrecommendation systems. In WWW, pages 278-288, 2015.\n[7] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol,\nP. Vincent, and S. Bengio. Why does unsupervised\npre-training help deep learning? Journal of Machine\nLearning Research, 11:625-660, 2010.\n[8] X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning\nimage and user features for recommendation in social\nnetworks. In ICCV, pages 4274-4282, 2015.\n[9] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier\nneural networks. In AISTATS, pages 315-323, 2011.\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\nlearning for image recognition. In CVPR, 2016.\n[11] X. He, T. Chen, M.-Y. Kan, and X. Chen. TriRank:\nReview-aware explainable recommendation by modeling\naspects. In CIKM, pages 1661-1670, 2015.\n[12] X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama.\nPredicting the popularity of web 2.0 items based on user\ncomments. In SIGIR, pages 233-242, 2014.\n[13] X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based\nmulti-view clustering of web 2.0 items. In WWW, pages\n771-782, 2014.\n[14] X. He, H. Zhang, M.-Y. Kan, and T.-S. Chua. Fast matrix\nfactorization for online recommendation with implicit\nfeedback. In SIGIR, pages 549-558, 2016.\n[15] R. Hong, Z. Hu, L. Liu, M. Wang, S. Yan, and Q. Tian.\nUnderstanding blooming human groups in social networks.\nIEEE Transactions on Multimedia, 17(11):1980-1988, 2015.\n[16] R. Hong, Y. Yang, M. Wang, and X. S. Hua. Learning\nvisual semantic relationships for efficient visual retrieval.\nIEEE Transactions on Big Data, 1(4):152-161, 2015.\n[17] K. Hornik, M. Stinchcombe, and H. White. Multilayer\nfeedforward networks are universal approximators. Neural\nNetworks, 2(5):359-366, 1989.\n[18] L. Hu, A. Sun, and Y. Liu. Your neighbors affect your\nratings: On geographical neighborhood influence to rating\nprediction. In SIGIR, pages 345-354, 2014.\n[19] Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering\nfor implicit feedback datasets. In ICDM, pages 263-272,\n2008.\n[20] D. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR, pages 1-15, 2014.\n[21] Y. Koren. Factorization meets the neighborhood: A\nmultifaceted collaborative filtering model. In KDD, pages\n426-434, 2008.\n[22] S. Li, J. Kawale, and Y. Fu. Deep collaborative filtering via\nmarginalized denoising auto-encoder. In CIKM, pages\n811-820, 2015.\n[23] D. Liang, L. Charlin, J. McInerney, and D. M. Blei.\nModeling user exposure in recommendation. In WWW,\npages 951-961, 2016.\n[24] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich. A\nreview of relational machine learning for knowledge graphs.\nProceedings of the IEEE, 104:11-33, 2016.\n[25] X. Ning and G. Karypis. Slim: Sparse linear methods for\ntop-n recommender systems. In ICDM, pages 497-506,\n2011.\n[26] S. Rendle. Factorization machines. In ICDM, pages\n995-1000, 2010.\n[27] S. Rendle, C. Freudenthaler, Z. Gantner, and\nL. Schmidt-Thieme. Bpr: Bayesian personalized ranking\nfrom implicit feedback. In UAI, pages 452-461, 2009.\n[28] S. Rendle, Z. Gantner, C. Freudenthaler, and</zone>\n  <zone label=\"GEN_OTHER\">182</zone>\n  <zone label=\"GEN_REFERENCES\">L. Schmidt-Thieme. Fast context-aware recommendations\nwith factorization machines. In SIGIR, pages 635-644,\n2011.\n[29] R. Salakhutdinov and A. Mnih. Probabilistic matrix\nfactorization. In NIPS, pages 1-8, 2008.\n[30] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted\nboltzmann machines for collaborative filtering. In ICDM,\npages 791-798, 2007.\n[31] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.\nItem-based collaborative filtering recommendation\nalgorithms. In WWW, pages 285-295, 2001.\n[32] S. Sedhain, A. K. Menon, S. Sanner, and L. Xie. Autorec:\nAutoencoders meet collaborative filtering. In WWW, pages\n111-112, 2015.\n[33] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning\nwith neural tensor networks for knowledge base completion.\nIn NIPS, pages 926-934, 2013.\n[34] N. Srivastava and R. R. Salakhutdinov. Multimodal\nlearning with deep boltzmann machines. In NIPS, pages\n2222-2230, 2012.\n[35] F. Strub and J. Mary. Collaborative filtering with stacked\ndenoising autoencoders and sparse inputs. In NIPS\nWorkshop on Machine Learning for eCommerce, 2015.\n[36] T. T. Truyen, D. Q. Phung, and S. Venkatesh. Ordinal\nboltzmann machines for collaborative filtering. In UAI,\npages 548-556, 2009.\n[37] A. Van den Oord, S. Dieleman, and B. Schrauwen. Deep\ncontent-based music recommendation. In NIPS, pages\n2643-2651, 2013.\n[38] H. Wang, N. Wang, and D.-Y. Yeung. Collaborative deep\nlearning for recommender systems. In KDD, pages\n1235-1244, 2015.\n[39] M. Wang, W. Fu, S. Hao, D. Tao, and X. Wu. Scalable\nsemi-supervised learning by efficient anchor graph\nregularization. IEEE Transactions on Knowledge and Data\nEngineering, 28(7):1864-1877, 2016.\n[40] M. Wang, H. Li, D. Tao, K. Lu, and X. Wu. Multimodal\ngraph-based reranking for web image search. IEEE\nTransactions on Image Processing, 21(11):4649-4661, 2012.\n[41] M. Wang, X. Liu, and X. Wu. Visual classification by l1\nhypergraph modeling. IEEE Transactions on Knowledge\nand Data Engineering, 27(9):2564-2574, 2015.\n[42] X. Wang, L. Nie, X. Song, D. Zhang, and T.-S. Chua.\nUnifying virtual and physical worlds: Learning towards\nlocal and global consistency. ACM Transactions on\nInformation Systems, 2017.\n[43] X. Wang and Y. Wang. Improving content-based and\nhybrid music recommendation using deep learning. In MM,\npages 627-636, 2014.\n[44] Y. Wu, C. DuBois, A. X. Zheng, and M. Ester.\nCollaborative denoising auto-encoders for top-n\nrecommender systems. In WSDM, pages 153-162, 2016.\n[45] F. Zhang, N. J. Yuan, D. Lian, X. Xie, and W.-Y. Ma.\nCollaborative knowledge base embedding for recommender\nsystems. In KDD, pages 353-362, 2016.\n[46] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S.\nChua. Discrete collaborative filtering. In SIGIR, pages\n325-334, 2016.\n[47] H. Zhang, Y. Yang, H. Luan, S. Yang, and T.-S. Chua.\nStart from scratch: Towards automatically identifying,\nmodeling, and naming visual attributes. In MM, pages\n187-196, 2014.\n[48] Y. Zheng, B. Tang, W. Ding, and H. Zhou. A neural\nautoregressive approach to collaborative filtering. In ICML,\npages 764-773, 2016.</zone>\n</document>"}