{"content": "<document>\n  <zone label=\"MET_BIB_INFO\">Hindawi Publishing Corporation\nAdvances in Artificial Intelligence\nVolume 2009, Article ID 421425, 19 pages\ndoi:10.1155/2009/421425</zone>\n  <zone label=\"MET_TITLE\">Review Article\nA Survey of Collaborative Filtering Techniques</zone>\n  <zone label=\"MET_AUTHOR\">Xiaoyuan Su and Taghi M. Khoshgoftaar</zone>\n  <zone label=\"MET_AFFILIATION\">Department of Computer Science and Engineering, Florida Atlantic University, 777 Glades Road, Boca Raton, FL 33431, USA</zone>\n  <zone label=\"MET_CORRESPONDENCE\">Correspondence should be addressed to Xiaoyuan Su, suxiaoyuan@gmail.com</zone>\n  <zone label=\"MET_DATES\">Received 9 February 2009; Accepted 3 August 2009</zone>\n  <zone label=\"GEN_OTHER\">Recommended by Jun Hong</zone>\n  <zone label=\"MET_ABSTRACT\">As one of the most successful approaches to building recommender systems, collaborative filtering (CF) uses the known preferences\nof a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first\nintroduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy\nprotection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, modelbased,\nand hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative\nalgorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic\ntechniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a\nroadmap for research and practice in this area.</zone>\n  <zone label=\"MET_COPYRIGHT\">Copyright \u00a9 2009 X. Su and T. M. Khoshgoftaar. This is an open access article distributed under the Creative Commons\nAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is\nproperly cited.</zone>\n  <zone label=\"BODY_HEADING\">1. Introduction</zone>\n  <zone label=\"BODY_CONTENT\">In everyday life, people rely on recommendations from\nother people by spoken words, reference letters, news reports\nfrom news media, general surveys, travel guides, and so\nforth. Recommender systems assist and augment this natural\nsocial process to help people sift through available books,\narticles, webpages, movies, music, restaurants, jokes, grocery\nproducts, and so forth to find the most interesting and\nvaluable information for them. The developers of one of\nthe first recommender systems, Tapestry [1] (other earlier\nrecommendation systems include rule-based recommenders\nand user-customization), coined the phrase \u201ccollaborative\nfiltering (CF),\u201d which has been widely adopted regardless of\nthe facts that recommenders may not explicitly collaborate\nwith recipients and recommendations may suggest particularly\ninteresting items, in addition to indicating those that\nshould be filtered out [2]. The fundamental assumption\nof CF is that if users X and Y rate n items similarly, or\nhave similar behaviors (e.g., buying, watching, listening), and\nhence will rate or act on other items similarly [3].\nCF techniques use a database of preferences for items\nby users to predict additional topics or products a new user\nmight like. In a typical CF scenario, there is a list of m users\n{u1, u2, . . . , um} and a list of n items {i1, i2, . . . , in}, and each\nuser, ui, has a list of items, Iui, which the user has rated,\nor about which their preferences have been inferred through\ntheir behaviors. The ratings can either be explicit indications,\nand so forth, on a 1-5 scale, or implicit indications, such\nas purchases or click-throughs [4]. For example, we can\nconvert the list of people and the movies they like or dislike\n(Table 1(a)) to a user-item ratings matrix (Table 1(b)),\nin which Tony is the active user that we want to make\nrecommendations for. There are missing values in the matrix\nwhere users did not give their preferences for certain items.\nThere are many challenges for collaborative filtering\ntasks (Section 2). CF algorithms are required to have the\nability to deal with highly sparse data, to scale with the\nincreasing numbers of users and items, to make satisfactory\nrecommendations in a short time period, and to deal with\nother problems like synonymy (the tendency of the same or\nsimilar items to have different names), shilling attacks, data\nnoise, and privacy protection problems.\nEarly generation collaborative filtering systems, such as\nGroupLens [5], use the user rating data to calculate the similarity\nor weight between users or items and make predictions\nor recommendations according to those calculated similarity\nvalues. The so-called memory-based CF methods (Section 3)\nare notably deployed into commercial systems such as\nhttp://www.amazon.com/ (see an example in Figure 1) and</zone>\n  <zone label=\"GEN_OTHER\">2</zone>\n  <zone label=\"MET_ABSTRACT\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">Table 1: An example of a user-item matrix.\n(a)\n(b)\nAlice: (like) Shrek, Snow White, (dislike) Superman\nBob: (like) Snow White, Superman, (dislike) spiderman\nChris: (like) spiderman, (dislike) Snow white\nTony: (like) Shrek, (dislike) Spiderman\nShrek Snow White Spider-man Super-man\nAlice Like\nBob\nChris\nTony Like\nLike\nLike\nDislike\nDislike\nLike\nDislike\nDislike\nLike\n?\nBarnes and Noble, because they are easy-to-implement and\nhighly effective [6, 7]. Customization of CF systems for each\nuser decreases the search effort for users. It also promises\na greater customer loyalty, higher sales, more advertising\nrevenues, and the benefit of targeted promotions [8].\nHowever, there are several limitations for the memorybased\nCF techniques, such as the fact that the similarity\nvalues are based on common items and therefore are\nunreliable when data are sparse and the common items are\ntherefore few. To achieve better prediction performance and\novercome shortcomings of memory-based CF algorithms,\nmodel-based CF approaches have been investigated. Modelbased\nCF techniques (Section 4) use the pure rating data\nto estimate or learn a model to make predictions [9]. The\nmodel can be a data mining or machine learning algorithm.\nWell-known model-based CF techniques include Bayesian\nbelief nets (BNs) CF models [9-11], clustering CF models\n[12, 13], and latent semantic CF models [7]. An MDP\n(Markov decision process)-based CF system [14] produces\na much higher profit than a system that has not deployed the\nrecommender.\nBesides collaborative filtering, content-based filtering is\nanother important class of recommender systems. Contentbased\nrecommender systems make recommendations by\nanalyzing the content of textual information and finding\nregularities in the content. The major difference between\nCF and content-based recommender systems is that CF\nonly uses the user-item ratings data to make predictions\nand recommendations, while content-based recommender\nsystems rely on the features of users and items for predictions\n[15]. Both content-based recommender systems and CF\nsystems have limitations. While CF systems do not explicitly\nincorporate feature information, content-based systems do\nnot necessarily incorporate the information in preference\nsimilarity across individuals [8].\nHybrid CF techniques, such as the content-boosted CF\nalgorithm [16] and Personality Diagnosis (PD) [17], combine\nCF and content-based techniques, hoping to avoid\nthe limitations of either approach and thereby improve\nrecommendation performance (Section 5).\nA brief overview of CF techniques is depicted in Table 2.\nFigure 1: Amazon recommends products to customers by customizing\nCF systems.\nTo evaluate CF algorithms (Section 6), we need to use\nmetrics according to the types of CF application. Instead\nof classification error, the most widely used evaluation\nmetric for prediction performance of CF is Mean Absolute\nError (MAE). Precision and recall are widely used metrics\nfor ranked lists of returned items in information retrieval\nresearch. ROC sensitivity is often used as a decision support\naccuracy metric.\nAs drawing convincing conclusions from artificial data is\nrisky, data from live experiments are more desirable for CF\nresearch. The commonly used CF databases are MovieLens\n[18], Jester [19], and Netflix prize data [20]. In Section 7, we\ngive the conclusion and discussion of this work.</zone>\n  <zone label=\"BODY_HEADING\">2. Characteristics and Challenges of\nCollaborative Filtering</zone>\n  <zone label=\"BODY_CONTENT\">E-commerce recommendation algorithms often operate\nin a challenging environment, especially for large online\nshopping companies like eBay and Amazon. Usually, a\nrecommender system providing fast and accurate recommendations\nwill attract the interest of customers and bring\nbenefits to companies. For CF systems, producing highquality\npredictions or recommendations depends on how\nwell they address the challenges, which are characteristics of\nCF tasks as well.\n2.1. Data Sparsity. In practice, many commercial recommender\nsystems are used to evaluate very large product\nsets. The user-item matrix used for collaborative filtering\nwill thus be extremely sparse and the performances of the\npredictions or recommendations of the CF systems are\nchallenged.\nThe data sparsity challenge appears in several situations,\nspecifically, the cold start problem occurs when a new user\nor item has just entered the system, it is difficult to find\nsimilar ones because there is not enough information (in\nsome literature, the cold start problem is also called the\nnew user problem or new item problem [21, 22]). New items\ncannot be recommended until some users rate it, and new</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"GEN_OTHER\">3</zone>\n  <zone label=\"BODY_CONTENT\">Table 2: Overview of collaborative filtering techniques.\nCF categories\nMemory-based CF\nModel-based CF\nHybrid recommenders\nRepresentative techniques\n\u2217Neighbor-based CF\n(item-based/user-based CF\nalgorithms with Pearson/vector\ncosine correlation)\n\u2217Item-based/user-based top-N\nrecommendations\n\u2217Bayesian belief nets CF\n\u2217clustering CF\n\u2217MDP-based CF\n\u2217latent semantic CF\n\u2217sparse factor analysis\n\u2217CF using dimensionality\nreduction techniques, for example,\nSVD, PCA\n\u2217content-based CF recommender,\nfor example, Fab\n\u2217content-boosted CF\n\u2217hybrid CF combining\nmemory-based and model-based\nCF algorithms, for example,\nPersonality Diagnosis\nMain advantages\n\u2217easy implementation\n\u2217new data can be added easily and\nincrementally\n\u2217need not consider the content of\nthe items being recommended\n\u2217scale well with co-rated items\n\u2217better address the sparsity,\nscalability and other problems\n\u2217improve prediction performance\n\u2217give an intuitive rationale for\nrecommendations\n\u2217overcome limitations of CF and\ncontent-based or other\nrecommenders\n\u2217improve prediction performance\n\u2217overcome CF problems such as\nsparsity and gray sheep\nMain shortcomings\n\u2217are dependent on human ratings\n\u2217performance decrease when data\nare sparse\n\u2217cannot recommend for new users\nand items\n\u2217have limited scalability for large\ndatasets\n\u2217expensive model-building\n\u2217have trade-off between prediction\nperformance and scalability\n\u2217lose useful information for\ndimensionality reduction\ntechniques\n\u2217have increased complexity and\nexpense for implementation\n\u2217need external information that\nusually not available\nusers are unlikely given good recommendations because of\nthe lack of their rating or purchase history. Coverage can\nbe defined as the percentage of items that the algorithm\ncould provide recommendations for. The reduced coverage\nproblem occurs when the number of users' ratings may be\nvery small compared with the large number of items in\nthe system, and the recommender system may be unable to\ngenerate recommendations for them. Neighbor transitivity\nrefers to a problem with sparse databases, in which users\nwith similar tastes may not be identified as such if they\nhave not both rated any of the same items. This could\nreduce the effectiveness of a recommendation system which\nrelies on comparing users in pairs and therefore generating\npredictions.\nTo alleviate the data sparsity problem, many approaches\nhave been proposed. Dimensionality reduction techniques,\nsuch as Singular Value Decomposition (SVD) [23], remove\nunrepresentative or insignificant users or items to reduce\nthe dimensionalities of the user-item matrix directly. The\npatented Latent Semantic Indexing (LSI) used in information\nretrieval is based on SVD [24, 25], in which similarity\nbetween users is determined by the representation of the\nusers in the reduced space. Goldberg et al. [3] developed\neigentaste, which applies Principle Component Analysis\n(PCA), a closely-related factor analysis technique first\ndescribed by Pearson in 1901 [26], to reduce dimensionality.\nHowever, when certain users or items are discarded, useful\ninformation for recommendations related to them may\nget lost and recommendation quality may be degraded [6,\n27].\nHybrid CF algorithms, such as the content-boosted CF\nalgorithm [16], are found helpful to address the sparsity\nproblem, in which external content information can be used\nto produce predictions for new users or new items. In Ziegler\net al. [28], a hybrid collaborative filtering approach was\nproposed to exploit bulk taxonomic information designed\nfor exact product classification to address the data sparsity\nproblem of CF recommendations, based on the generation\nof profiles via inference of super-topic score and topic\ndiversification [28]. Schein et al. proposed the aspect model\nlatent variable method for cold start recommendation, which\ncombines both collaborative and content information in\nmodel fitting [29]. Kim and Li proposed a probabilistic\nmodel to address the cold start problem, in which items are\nclassified into groups and predictions are made for users\nconsidering the Gaussian distribution of user ratings [30].\nModel-based CF algorithms, such as TAN-ELR (tree augmented\nna\u00a8\u0131ve Bayes optimized by extended logistic regression)\n[11, 31], address the sparsity problem by providing\nmore accurate predictions for sparse data. Some new modelbased\nCF techniques that tackle the sparsity problem include\nthe association retrieval technique, which applies an associative\nretrieval framework and related spreading activation\nalgorithms to explore transitive associations among users\nthrough their rating and purchase history [32]; Maximum\nmargin matrix factorizations (MMMF), a convex, infinite\ndimensional alternative to low-rank approximations and\nstandard factor models [33, 34]; ensembles of MMMF\n[35]; multiple imputation-based CF approaches [36]; and\nimputation-boosted CF algorithms [37].</zone>\n  <zone label=\"GEN_OTHER\">4</zone>\n  <zone label=\"BODY_CONTENT\">2.2. Scalability. When numbers of existing users and items\ngrow tremendously, traditional CF algorithms will suffer\nserious scalability problems, with computational resources\ngoing beyond practical or acceptable levels. For example,\nwith tens of millions of customers (M) and millions of\ndistinct catalog items (N ), a CF algorithm with the complexity\nof O(n) is already too large. As well, many systems\nneed to react immediately to online requirements and make\nrecommendations for all users regardless of their purchases\nand ratings history, which demands a high scalability of a CF\nsystem [6].\nDimensionality reduction techniques such as SVD can\ndeal with the scalability problem and quickly produce\ngood quality recommendations, but they have to undergo\nexpensive matrix factorization steps. An incremental SVD\nCF algorithm [38] precomputes the SVD decomposition\nusing existing users. When a new set of ratings are added\nto the database, the algorithm uses the folding-in projection\ntechnique [25, 39] to build an incremental system without recomputing\nthe low-dimensional model from scratch. Thus it\nmakes the recommender system highly scalable.\nMemory-based CF algorithms, such as the item-based\nPearson correlation CF algorithm can achieve satisfactory\nscalability. Instead of calculating similarities between all pairs\nof items, item-based Pearson CF calculates the similarity\nonly between the pair of co-rated items by a user [6,\n40]. A simple Bayesian CF algorithm tackles the scalability\nproblem by making predictions based on observed ratings\n[41]. Model-based CF algorithms, such as clustering CF\nalgorithms, address the scalability problem by seeking users\nfor recommendation within smaller and highly similar\nclusters instead of the entire database [13, 42-44], but there\nare tradeoffs between scalability and prediction performance.\n2.3. Synonymy. Synonymy refers to the tendency of a\nnumber of the same or very similar items to have different\nnames or entries. Most recommender systems are unable\nto discover this latent association and thus treat these\nproducts differently. For example, the seemingly different\nitems \u201cchildren movie\u201d and \u201cchildren film\u201d are actual the\nsame item, but memory-based CF systems would find no\nmatch between them to compute similarity. Indeed, the\ndegree of variability in descriptive term usage is greater than\ncommonly suspected. The prevalence of synonyms decreases\nthe recommendation performance of CF systems.\nPrevious attempts to solve the synonymy problem\ndepended on intellectual or automatic term expansion, or\nthe construction of a thesaurus. The drawback for fully\nautomatic methods is that some added terms may have\ndifferent meanings from intended, thus leading to rapid\ndegradation of recommendation performance [45].\nThe SVD techniques, particularly the Latent Semantic\nIndexing (LSI) method, are capable of dealing with the\nsynonymy problems. SVD takes a large matrix of termdocument\nassociation data and construct a semantic space\nwhere terms and documents that are closely associated are\nplaced closely to each other. SVD allows the arrangement of\nthe space to reflect the major associative patterns in the data,</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">and ignore the smaller, less important ones. The performance\nof LSI in addressing the synonymy problem is impressive\nat higher recall levels where precision is ordinarily quite\nlow, thus representing large proportional improvements.\nHowever, the performance of the LSI method at the lowest\nlevels of recall is poor [25].\nThe LSI method gives only a partial solution to the\npolysemy problem, which refers to the fact that most words\nhave more than one distinct meaning [25].\n2.4. Gray Sheep. Gray sheep refers to the users whose\nopinions do not consistently agree or disagree with any group\nof people and thus do not benefit from collaborative filtering\n[46]. Black sheep are the opposite group whose idiosyncratic\ntastes make recommendations nearly impossible. Although\nthis is a failure of the recommender system, non-electronic\nrecommenders also have great problems in these cases, so\nblack sheep is an acceptable failure [47].\nClaypool et al. provided a hybrid approach combining\ncontent-based and CF recommendations by basing a prediction\non a weighted average of the content-based prediction\nand the CF prediction. In that approach, the weights of the\ncontent-based and CF predictions are determined on a peruser\nbasis, allowing the system to determine the optimal\nmix of content-based and CF recommendation for each user,\nhelping to solve the gray sheep problem [46].\n2.5. Shilling Attacks. In cases where anyone can provide\nrecommendations, people may give tons of positive recommendations\nfor their own materials and negative recommendations\nfor their competitors. It is desirable for CF\nsystems to introduce precautions that discourage this kind of\nphenomenon [2].\nRecently, the shilling attacks models for collaborative\nfiltering system have been identified and their effectiveness\nhas been studied. Lam and Riedl found that item-based\nCF algorithm was much less affected by the attacks than\nthe user-based CF algorithm, and they suggest that new\nways must be used to evaluate and detect shilling attacks on\nrecommender systems [48]. Attack models for shilling the\nitem-based CF systems have been examined by Mobasher\net al., and alternative CF systems such as hybrid CF systems\nand model-based CF systems were believed to have the ability\nto provide partial solutions to the bias injection problem\n[49]. O'Mahony et al. contributed to solving the shilling\nattacks problem by analyzing robustness, a recommender\nsystem's resilience to potentially malicious perturbations in\nthe customer/product rating matrix [50].\nBell and Koren [51] used a comprehensive approach to\nthe shilling attacks problem by removing global effects in\nthe data normalization stage of the neighbor-based CF, and\nworking with residual of global effects to select neighbors.\nThey achieved improved CF performance on the Netflix [20]\ndata.\n2.6. Other Challenges. As people may not want their habits\nor views widely known, CF systems also raise concerns about</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"GEN_OTHER\">5</zone>\n  <zone label=\"BODY_CONTENT\">Table 3: The Nexflix Prize Leaderboard as of July 2009.\nRank Team\nBest RMSE score Improvement (%)\nneighborhood models [58]. Some interesting research papers\non the Netflix prize challenge can be found in the 2008 KDD\nNetflix Workshop (http://netflixkddworkshop2008.info/).\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nU1\nU2\nU3\nU4\nU5\nBellKor's Pragmatic\nChaos\nGrand Prize Team\nOpera Solutions and\nVandelay United\nVandelay Industries!\nPragmatic Theory\nBellKor in BigChaos\nDace\nOpera Solutions\nBellKor\nBigChaos\nI1\n4\n4\n3\n4\n2\nI3\n5\n1\n2\n3\n10.07\n9.91\n9.89\n9.83\n9.80\n9.71\n9.55\n9.49\n9.48\n9.47\nI4\n5\n4\n5\nTable 4: A simple example of ratings matrix.\npersonal privacy. Miller et al. [4] and Canny [52] find ways\nto protect users' privacy for CF recommendation tasks.\nIncreased noise (or sabotage) is another challenge, as\nthe user population becomes more diverse. Ensembles of\nmaximum margin matrix factorizations [35] and instance\nselection techniques [53] are found useful to address the\nnoise problems of CF tasks. As Dempster-Shafer (DS)\ntheory [54, 55] and imputation techniques [56] have been\nsuccessfully applied to accommodate imperfect and noisy\ndata for knowledge representation and classification tasks,\nthey are also potentially useful to deal with the noise problem\nof CF tasks.\nExplainability is another important aspect of recommender\nsystems. An intuitive reasoning such as \u201cyou will like\nthis book because you liked those books\u201d will be appealing\nand beneficial to readers, regardless of the accuracy of the\nexplanations [57].\n2.7. The Netflix Prize Challenge. Launched in October 2006,\nthe Netflix prize challenge [20] attracted thousands of\nresearchers to compete in the million-dollar-prize race for\na most improved performance for movie recommendations.\nThe challenge is featured with a large-scale industrial\ndataset (with 480,000 users and 17,770 movies), and a rigid\nperformance metric of RMSE (see detailed description in\nSection 6).\nUp to July, 2009, the Leaderboard on the Netflix prize\ncompetition is as Table 3, in which the leading team \u201cBellKor\nin Pragmatic Chaos\u201d (with 10.05% improved RMSE over the\nNetflix movie recommendation system: Cinematch) based\ntheir solution on a merged model of latent factor and\nI2\n?\n2\n4\n1\n0.8556\n0.8571\n0.8573\n0.8579\n0.8582\n0.8590\n0.8605\n0.8611\n0.8612\n0.8613</zone>\n  <zone label=\"BODY_HEADING\">3. Memory-Based Collaborative\nFiltering Techniques</zone>\n  <zone label=\"BODY_CONTENT\">Memory-based CF algorithms use the entire or a sample of\nthe user-item database to generate a prediction. Every user is\npart of a group of people with similar interests. By identifying\nthe so-called neighbors of a new user (or active user), a\nprediction of preferences on new items for him or her can\nbe produced.\nThe neighborhood-based CF algorithm, a prevalent\nmemory-based CF algorithm, uses the following steps: calculate\nthe similarity or weight, wi,j , which reflects distance,\ncorrelation, or weight, between two users or two items, i\nand j; produce a prediction for the active user by taking the\nweighted average of all the ratings of the user or item on\na certain item or user, or using a simple weighted average\n[40]. When the task is to generate a top-N recommendation,\nwe need to find k most similar users or items (nearest\nneighbors) after computing the similarities, then aggregate\nthe neighbors to get the top-N most frequent items as the\nrecommendation.\n3.1. Similarity Computation. Similarity computation between\nitems or users is a critical step in memory-based\ncollaborative filtering algorithms. For item-based CF algorithms,\nthe basic idea of the similarity computation between\nitem i and item j is first to work on the users who have\nrated both of these items and then to apply a similarity\ncomputation to determine the similarity, wi,j , between the\ntwo co-rated items of the users [40]. For a user-based CF\nalgorithm, we first calculate the similarity, wu,v, between the\nusers u and v who have both rated the same items.\nThere are many different methods to compute similarity\nor weight between users or items.\n3.1.1. Correlation-Based Similarity. In this case, similarity\nwu,v between two users u and v, or wi,j between two items\ni and j, is measured by computing the Pearson correlation or\nother correlation-based similarities.\nPearson correlation measures the extent to which two\nvariables linearly relate with each other [5]. For the userbased\nalgorithm, the Pearson correlation between users u and\nv is\nwu,v =\ni\u2208I ru,i \u2212 ru rv,i \u2212 rv\n2\ni\u2208I ru,i \u2212 ru\ni\u2208I rv,i \u2212 rv\n2</zone>\n  <zone label=\"GEN_OTHER\">,</zone>\n  <zone label=\"BODY_CONTENT\">(1)\nwhere the i \u2208 I summations are over the items that both the\nusers u and v have rated and ru is the average rating of the\nco-rated items of the uth user. In an example in Table 4, we\nhave w1,5 = 0.756.</zone>\n  <zone label=\"GEN_OTHER\">6\n1</zone>\n  <zone label=\"BODY_CONTENT\">2\n...\nl\n...\nn \u2212 1\nn\n1 2 \u00b7 \u00b7 \u00b7 i\nR\nj \u00b7 \u00b7 \u00b7 m \u2212 1 m\n?\nR\nR\n?\nR\nR\nR\nR\nR\nFigure 2: item-based similarity (wi,j ) calculation based on the corated\nitems i and j from users 2, l and n.\nFor the item-based algorithm, denote the set of users u \u2208\nU who rated both items i and j, then the Pearson Correlation\nwill be\nwi,j =\nu\u2208U ru,i \u2212 ri ru,j \u2212 r j\nu\u2208U (ru,i \u2212 ri)2\nu\u2208U (ru,j \u2212 r j )2\n,\n(2)\nwhere ru,i is the rating of user u on item i, ri is the average\nrating of the ith item by those users, see Figure 2 [40].\nSome variations of item-based and user-based Pearson\ncorrelations can be found in [59]. The Pearson correlationbased\nCF algorithm is a representative CF algorithm, and is\nwidely used in the CF research community.\nOther correlation-based similarities include: constrained\nPearson correlation, a variation of Pearson correlation that\nuses midpoint instead of mean rate; Spearman rank correlation,\nsimilar to Pearson correlation, except that the ratings are\nranks; and Kendall's \u03c4 correlation, similar to the Spearman\nrank correlation, but instead of using ranks themselves, only\nthe relative ranks are used to calculate the correlation [3,\n60].\nUsually the number of users in the computation of\nsimilarity is regarded as the neighborhood size of the active\nuser, and similarity based CF is deemed as neighborhoodbased\nCF.\n3.1.2. Vector Cosine-Based Similarity. The similarity between\ntwo documents can be measured by treating each document\nas a vector of word frequencies and computing the cosine\nof the angle formed by the frequency vectors [61]. This\nformalism can be adopted in collaborative filtering, which\nuses users or items instead of documents and ratings instead\nof word frequencies.\nFormally, if R is the m \u00d7 n user-item matrix, then the\nsimilarity between two items, i and j, is defined as the cosine\nof the n dimensional vectors corresponding to the ith and jth\ncolumn of matrix R.\nVector cosine similarity between items i and j is given by\nwi,j = cos i, j\n=\n(3)\ni \u2022 j\ni \u2217 j\n,</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">where \u201c\u2022\u201d denotes the dot-product of the two vectors. To\nget the desired similarity computation, for n items, an n \u00d7 n\nsimilarity matrix is computed [27]. For example, if the vector\nA = {x1, y1}, vector B = {x2, y2}, the vector cosine similarity\nbetween A and B is\nwA,B = cos A, B\n=\nA\u2022B\nA \u2217 B\n=\nx1x2 + y1 y2\nx12 + y12 x22 + y22\n.\nIn an actual situation, different users may use different\nrating scales, which the vector cosine similarity cannot take\ninto account. To address this drawback, adjusted cosine similarity\nis used by subtracting the corresponding user average\nfrom each co-rated pair. The Adjusted cosine similarity has\nthe same formula as Pearson correlation (2). In fact, Pearson\ncorrelation performs cosine similarity with some sort of\nnormalization of the user's ratings according to his own\nrating behavior. Hence, we may get negative values with\nPearson correlation, but not with cosine similarity, supposing\nwe have an n-point rating scale.\n3.1.3. Other Similarities. Another similarity measure is conditional\nprobability-based similarity [62, 63]. As it is not\ncommonly-used, we will not discuss it in detail in this paper.\n3.2. Prediction and Recommendation Computation. To obtain\npredictions or recommendations is the most important step\nin a collaborative filtering system. In the neighborhoodbased\nCF algorithm, a subset of nearest neighbors of the\nactive user are chosen based on their similarity with him\nor her, and a weighted aggregate of their ratings is used to\ngenerate predictions for the active user [64].\n3.2.1. Weighted Sum of Others' Ratings. To make a prediction\nfor the active user, a, on a certain item, i, we can take a\nweighted average of all the ratings on that item according to\nthe following formula [5]:\nPa,i = ra +\nu\u2208U ru,i \u2212 ru \u00b7 wa,u ,\nu\u2208U wa,u\nwhere ra and ru are the average ratings for the user a and\nuser u on all other rated items, and wa,u is the weight between\nthe user a and user u. The summations are over all the users\nu \u2208 U who have rated the item i. For the simple example\nin Table 4, using the user-based CF algorithm, to predict the\nrating for U1 on I2, we have\nP1,2 = r1 +\nu ru,2 \u2212 ru \u00b7 w1,u\nu w1,u\nr2,2 \u2212 r2 w1,2 + r4,2 \u2212 r4 w1,4 + r5,2 \u2212 r5 w1,5\nw1,2 + w1,4 +\nw1,5\n(2 \u2212 2.5)(\u22121) + (4 \u2212 4)0 + (1 \u2212 3.33)0.756\n1 + 0 + 0.756\n(4)\n(5)\n(6)\n= r1 +\n= 4.67 +\n= 3.95.</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">Note the above prediction is based on the neighborhood\nof the active users.\n3.2.2. Simple Weighted Average. For item-based prediction,\nwe can use the simple weighted average to predict the rating,\nPu,i, for user u on item i [40]\nPu,i =\nn\u2208N ru,nwi,n ,\nn\u2208N wi,n\n(7)\nwhere the summations are over all other rated items n \u2208 N\nfor user u, wi,n is the weight between items i and n, ru,n is the\nrating for user u on item n.\n3.3. Top-N Recommendations. Top-N recommendation is to\nrecommend a set of N top-ranked items that will be of\ninterest to a certain user. For example, if you are a returning\ncustomer, when you log into your http://amazon.com/\naccount, you may be recommended a list of books (or other\nproducts) that may be of your interest (see Figure 1). TopN\nrecommendation techniques analyze the user-item matrix\nto discover relations between different users or items and\nuse them to compute the recommendations. Some models,\nsuch as association rule mining based models, can be used to\nmake top-N recommendations, which we will introduce in\nSection 4.\n3.3.1. User-Based Top-N Recommendation Algorithms. Userbased\ntop-N recommendation algorithms firstly identify the\nk most similar users (nearest neighbors) to the active user\nusing the Pearson correlation or vector-space model [9, 27], in\nwhich each user is treated as a vector in the m-dimensional\nitem space and the similarities between the active user and\nother users are computed between the vectors. After the k\nmost similar users have been discovered, their corresponding\nrows in the user-item matrix R are aggregated to identify\na set of items, C, purchased by the group together with\ntheir frequency. With the set C, user-based CF techniques\nthen recommend the top-N most frequent items in C\nthat the active user has not purchased. User-based topN\nrecommendation algorithms have limitations related to\nscalability and real-time performance [62].\n3.3.2. Item-Based Top-N Recommendation Algorithms. Itembased\ntop-N recommendation algorithms have been developed\nto address the scalability problem of user-based top-N\nrecommendation algorithms. The algorithms firstly compute\nthe k most similar items for each item according to the\nsimilarities; then identify the set, C, as candidates of recommended\nitems by taking the union of the k most similar items\nand removing each of the items in the set, U, that the user\nhas already purchased; then calculate the similarities between\neach item of the set C and the set U. The resulting set of\nthe items in C, sorted in decreasing order of the similarity,\nwill be the recommended item-based Top-N list [62]. One\nproblem of this method is, when the joint distribution\nof a set of items is different from the distributions of\nthe individual items in the set, the above schemes can</zone>\n  <zone label=\"GEN_OTHER\">7</zone>\n  <zone label=\"BODY_CONTENT\">potentially produce suboptimal recommendations. To solve\nthis problem, Deshpande and Karypis [63] developed higherorder\nitem-based top-N recommendation algorithms that\nuse all combinations of items up to a particular size when\ndetermining the itemsets to be recommended to a user.\n3.4. Extensions to Memory-Based Algorithms\n3.4.1. Default Voting. In many collaborative filters, pairwise\nsimilarity is computed only from the ratings in the\nintersection of the items both users have rated [5, 27]. It\nwill not be reliable when there are too few votes to generate\nsimilarity values. Also, focusing on intersection set similarity\nneglects the global rating behavior reflected in a user's entire\nrating history.\nEmpirically, assuming some default voting values for the\nmissing ratings can improve the CF prediction performance.\nHerlocker et al. [64] accounts for small intersection sets by\nreducing the weight of users that have fewer than 50 items in\ncommon. Chee et al. [13] uses the average of the clique (or\nsmall group) as default voting to extend each user's rating\nhistory. Breese et al. [9] uses a neutral or somewhat negative\npreference for the unobserved ratings and then computes the\nsimilarity between users on the resulting ratings data.\n3.4.2. Inverse User Frequency. The idea of inverse user frequency\n[61] applied in collaborative filtering is that universally\nliked items are not as useful in capturing similarity as\nless common items. The inverse frequency can be defined\nas f j = log(n/nj ), where nj is the number of users who\nhave rated item j and n is the total number of users. If\neveryone has rated item j, then f j is zero. To apply inverse\nuser frequency while using the vector similarity-based CF\nalgorithm, we need to use a transformed rating, which is\nsimply the original rating multiplied by the f j factor [9].\n3.4.3. Case Amplification. Case amplification refers to a\ntransform applied to the weights used in the basic collaborative\nfiltering prediction. The transform emphasizes high\nweights and punishes low weights [9]:\nwi,j = wi,j \u00b7 wi,j\n\u03c1\u22121</zone>\n  <zone label=\"GEN_OTHER\">,\n(8)</zone>\n  <zone label=\"BODY_CONTENT\">where \u03c1 is the case amplification power, \u03c1 \u2265 1, and a typical\nchoice of \u03c1 is 2.5 [65]. Case amplification reduces noise in the\ndata. It tends to favor high weights as small values raised to a\npower become negligible. If the weight is high, for example,\nwi,j = 0.9, then it remains high (0.92.5 \u2248 0.8); if it is low, for\nexample, wi,j = 0.1, then it will be negligible (0.12.5 \u2248 0.003).\n3.4.4. Imputation-Boosted CF Algorithms. When the rating\ndata for CF tasks are extremely sparse, it will be problematic\nto produce accurate predictions using the Pearson\ncorrelation-based CF. Su et al. [37, 66] proposed a framework\nof imputation-boosted collaborative filtering (IBCF), which\nfirst uses an imputation technique to fill in the missing\ndata, before using a traditional Pearson correlation-based\nCF algorithm on this completed data to predict a specific</zone>\n  <zone label=\"GEN_OTHER\">8</zone>\n  <zone label=\"BODY_CONTENT\">user rating for a specified item. After comprehensively\ninvestigating the use of various standard imputation techniques\n(including mean imputation, linear regression imputation,\nand predictive mean matching [67] imputation, and\nBayesian multiple imputation [68]), and machine learning\nclassifiers [66] (including na\u00a8\u0131ve Bayes, SVM, neural network,\ndecision tree, lazy Bayesian rules) as imputers for IBCF, they\nfound that the proposed IBCF algorithms can perform very\neffectively in general, and that IBCF using Bayesian multiple\nimputation, IBCF-NBM (a mixture IBCF which uses IBCF\nusing na\u00a8\u0131ve Bayes for denser datasets and IBCF using mean\nimputation for sparser ones) [37], and IBCF using na\u00a8\u0131ve\nBayes perform especially well, outperforming the contentboosted\nCF algorithm (a representative hybrid CF), and do\nso without using external content information.\n3.4.5. Weighted Majority Prediction. The weighted majority\nprediction algorithm proposed by Goldman and Warmuth\n[69] makes its prediction using the rows with observed data\nin the same column, weighted by the believed similarity\nbetween the rows, with binary rating values. The weights\n(or similarities, with initialized values of 1) are increased\nby multiplying it by (2 \u2212 \u03b3) when the compared values are\nsame, and decreased by multiplying by \u03b3 when different,\nwith \u03b3 \u2208 (0, 1). This update is equivalent to wii =\n(2 \u2212 \u03b3)Cii \u03b3Wii , where Cii is the number of rows that have\nthe same value as in row i and Wii is the number of rows\nhaving different values. The prediction for a rating on a\ncertain item by the active user is determined by the rating on\nthe item by a certain user, who has the highest accumulated\nweight value with the active user. This algorithm can be\ngeneralized to multiclass data, and be extended from user-touser\nsimilarity to item-to-item similarity and to user-itemcombined\nsimilarity [70]. One shortcoming of this algorithm\nis the scalability, when the user number or item number\ngrows over a certain large number n, it will be impractical\nfor the user-to-user or item-to-item similarity computations\nto update the O(n2) similarity matrices.</zone>\n  <zone label=\"BODY_HEADING\">4. Model-Based Collaborative\nFiltering Techniques</zone>\n  <zone label=\"BODY_CONTENT\">The design and development of models (such as machine\nlearning, data mining algorithms) can allow the system to\nlearn to recognize complex patterns based on the training\ndata, and then make intelligent predictions for the collaborative\nfiltering tasks for test data or real-world data,\nbased on the learned models. Model-based CF algorithms,\nsuch as Bayesian models, clustering models, and dependency\nnetworks, have been investigated to solve the shortcomings of\nmemory-based CF algorithms [9, 71]. Usually, classification\nalgorithms can be used as CF models if the user ratings are\ncategorical, and regression models and SVD methods and be\nused for numerical ratings.\n4.1. Bayesian Belief Net CF Algorithms. A Bayesian belief\nnet (BN) is a directed, acyclic graph (DAG) with a triplet\nN , A, \u0398 , where each node n \u2208 N represents a random</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">variable, each directed arc a \u2208 A between nodes is\na probabilistic association between variables, and \u0398 is a\nconditional probability table quantifying how much a node\ndepends on its parents [72]. Bayesian belief nets (BNs) are\noften used for classification tasks.\n4.1.1. Simple Bayesian CF Algorithm. The simple Bayesian\nCF algorithm uses a na\u00a8\u0131ve Bayes (NB) strategy to make predictions\nfor CF tasks. Assuming the features are independent\ngiven the class, the probability of a certain class given all of\nthe features can be computed, and then the class with the\nhighest probability will be classified as the predicted class\n[41]. For incomplete data, the probability calculation and\nclassification production are computed over observed data\n(the subscript o in the following equation indicates observed\nvalues):\nclass = arg max p classj\nj \u2208 classSet\no\nP Xo = xo | classj .\n(9)\nThe Laplace Estimator is used to smooth the probability\ncalculation and avoid a conditional probability of 0:\nP Xi = xi | Y = y =\n# Xi = xi, Y = y + 1\n# Y = y + |Xi|\n,\n(10)\nwhere |Xi| is the size of the class set {Xi}. For an example of\nbinary class, P(Xi = 0 | Y = 1) = 0/2 will be (0+1)/(2+2) =\n1/4, P(Xi = 1 | Y = 1) = 2/2 will be (2 + 1)/(2 + 2) = 3/4\nusing the Laplace Estimator.\nUsing the same example in Table 4, the class set is\n{1, 2, . . . , 5}, to produce the rating for U1 on I2 using the\nsimple Bayesian CF algorithm and the Laplace Estimator, we\nhave\nclass = arg max p cj | U2 = 2, U4 = 4, U5 = 1\ncj \u2208 {1,2,3,4,5}\n= arg max p cj P U2 = 2 | cj P U4 = 4 | cj\ncj\u2208{1,2,3,4,5}\n(11)\n\u00d7 P U5 = 1 | cj\n= arg max {0, 0, 0, 0.0031, 0.0019} = 4\ncj\u2208{1,2,3,4,5}\nin which p(5)P(U2 = 2 | 5)P(U4 = 4 | 5)P(U5 = 1 | 5) =\n(2/3) \u2217 (1/7) \u2217 (1/7) \u2217 (1/7) = 0.0019.\nIn Miyahara and Pazzani [10], multiclass data are firstly\nconverted to binary-class data, and then converted to a\nBoolean feature vector rating matrix. These conversions make\nthe use of the NB algorithm for CF tasks easier, but bring the\nproblems of scalability and the loss of multiclass information\nfor multiclass data. In Miyahara and Pazzani [41], they\napplied the simple Bayesian CF model only on binary data.\nBecause most real-world CF data are multiclass ones,\nSu and Khoshgoftaar [11] apply the simple Bayesian CF\nalgorithm to multiclass data for CF tasks, and found\nsimple Bayesian CF has worse predictive accuracy but better\nscalability than the Pearson correlation-based CF as it makes</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">predictions based on observed ratings, and the predictionmaking\nprocess is less time-consuming.\nThe simple Bayesian CF algorithm can be regarded\nas memory-based CF technique because of its in-memory\ncalculation for CF predictions. We put it in this section\nfor the reason that most other Bayesian CF algorithms are\nmodel-based CFs.\n4.1.2. NB-ELR and TAN-ELR CF Algorithms. Because of the\nlimitations of the simple Bayesian algorithm for CF tasks,\nadvanced BNs CF algorithms, with their ability to deal with\nincomplete data, can be used instead [11]. Extended logistic\nregression (ELR) is a gradient-ascent algorithm [31, 73],\nwhich is a discriminative parameter-learning algorithm that\nmaximizes log conditional likelihood.\nTAN-ELR and NB-ELR (tree augmented na\u00a8\u0131ve Bayes [74]\nand na\u00a8\u0131ve Bayes optimized by ELR, resp.) have been proven\nto have high classification accuracy for both complete and\nincomplete data [31, 73].\nApplied to CF tasks, working on real-world multiclass\nCF datasets and using MAE as evaluation criterion, the\nempirical results show that the TAN-ELR CF and NBELR\nCF algorithms perform significantly better than the\nsimple Bayesian CF algorithm, and consistently better than\nthe Pearson correlation memory-based CF algorithm [11].\nHowever, TAN-ELR and NB-ELR need a longer time to train\nthe models. A solution is to run the time-consuming training\nstage offline, and the online prediction-producing stage will\ntake a much shorter time.\n4.1.3. Other Bayesian CF Algorithms. Bayesian belief nets with\ndecision trees at each node. This model has a decision tree\nat each node of the BNs, where a node corresponds to each\nitem in the domain and the states of each node correspond to\nthe possible ratings for each item [9]. Their results show that\nthis model has similar prediction performance to Pearson\ncorrelation-based CF methods, and has better performance\nthan Bayesian-clustering and vector cosine memory-based\nCF algorithms.\nBaseline Bayesian model uses a Bayesian belief net with\nno arcs (baseline model) for collaborative filtering and recommends\nitems on their overall popularity [75]. However,\nthe performance is suboptimal.\n4.2. Clustering CF Algorithms. A cluster is a collection of\ndata objects that are similar to one another within the same\ncluster and are dissimilar to the objects in other clusters\n[76]. The measurement of the similarity between objects is\ndetermined using metrics such as Minkowski distance and\nPearson correlation.\nFor two data objects, X = (x1, x2, . . . , xn) and Y =\n(y1, y2, . . . , yn), the popular Minkowski distance is defined as\nd(X, Y ) = q\nn\ni=1\nxi \u2212 yi q,\n(12)\nwhere n is the dimension number of the object and xi, yi are\nthe values of the ith dimension of object X and Y respectively,</zone>\n  <zone label=\"GEN_OTHER\">9</zone>\n  <zone label=\"BODY_CONTENT\">and q is a positive integer. When q = 1, d is Manhattan\ndistance; when q = 2, d is Euclidian distance [76].\nClustering methods can be classified into three categories:\npartitioning methods, density-based methods, and\nhierarchical methods [76, 77]. A commonly-used partitioning\nmethod is k-means, proposed by MacQueen [78],\nwhich has two main advantages: relative efficiency and easy\nimplementation. Density-based clustering methods typically\nsearch for dense clusters of objects separated by sparse\nregions that represent noise. DBSCAN [79] and OPTICS\n[80] are well-known density-based clustering methods. Hierarchical\nclustering methods, such as BIRCH [81], create a\nhierarchical decomposition of the set of data objects using\nsome criterion.\nIn most situations, clustering is an intermediate step\nand the resulting clusters are used for further analysis or\nprocessing to conduct classification or other tasks. Clustering\nCF models can be applied in different ways. Sarwar et al. [43]\nand O'Connor and Herlocker [42] use clustering techniques\nto partition the data into clusters and use a memory-based\nCF algorithm such as a Pearson correlation-based algorithm\nto make predictions for CF tasks within each cluster.\nUsing the k-means method with k = 2, the RecTree\nmethod, proposed by Chee et al. [13], recursively splits\nthe originally large rating data into two sub-clusters as\nit constructs the RecTree from the root to its leaves. The\nresulting RecTree resembles an unbalanced binary tree, of\nwhich leaf nodes have a similarity matrix and internal nodes\nmaintain rating centroids of their subtrees. The prediction\nis made within the leaf node that the active user belongs to.\nRecTree scales by O(n log2(n)) for off-line recommendation\nand O(b) for on-line recommendation, where n is the dataset\nsize and b is the partition size, a constant, and it has an\nimproved accuracy over the Pearson correlation-based CF\nwhen selecting an appropriate size of advisors (cluster of\nusers).\nUngar and Foster [12] clusters users and items separately\nusing variations of k-means and Gibbs sampling [82], by\nclustering users based on the items they rated and clustering\nitems based on the users that rated them. Users can be\nreclustered based on the number of items they rated, and\nitems can be similarly re-clustered. Each user is assigned to\na class with a degree of membership proportional to the\nsimilarity between the user and the mean of the class. Their\nCF performance on synthetic data is good, but not good on\nreal data.\nA flexible mixture model (FMM) extends existing clustering\nalgorithms for CF by clustering both users and items\nat the same time, allowing each user and item to be in\nmultiple clusters and modeling the clusters of users and items\nseparately [15]. Experimental results show that the FMM\nalgorithm has better accuracy than the Pearson correlationbased\nCF algorithm and aspect model [83].\nClustering models have better scalability than typical\ncollaborative filtering methods because they make predictions\nwithin much smaller clusters rather than the entire\ncustomer base [13, 27, 44, 84]. The complex and expensive\nclustering computation is run offline. However, its\nrecommendation quality is generally low. It is possible to</zone>\n  <zone label=\"GEN_OTHER\">10</zone>\n  <zone label=\"BODY_CONTENT\">improve quality by using numerous fine-grained segments,\nbut then online user-segment classification becomes almost\nas expensive as finding similar customers using memorybased\ncollaborative filtering [6]. As optimal clustering over\nlarge data sets is impractical, most applications use various\nforms of greedy cluster generation techniques. For\nvery large datasets, especially those with high dimensionality,\nsampling or dimensionality reduction is also necessary.\n4.3. Regression-Based CF Algorithms. For memory-based CF\nalgorithms, in some cases, two rating vectors may be distant\nin terms of Euclidean distances but they have very high\nsimilarity using vector cosine or Pearson correlation measures,\nwhere memory-based CF algorithms do not fit well and need\nbetter solutions. Also, numerical ratings are common in reallife\nrecommender systems. Regression methods that are good\nat making predictions for numerical values are helpful to\naddress these problems.\nA regression method uses an approximation of the\nratings to make predictions based on a regression model. Let\nX = (X1, X2, . . . , Xn) be a random variable representing a\nuser's preferences on different items. The linear regression\nmodel can be expressed as\nY = \u039bX + N ,\n(13)\nwhere \u039b is a n \u00d7 k matrix. N = (N1, . . . , Nn) is a random\nvariable representing noise in user choices, Y is an n \u00d7 m\nmatrix with Yij is the rating of user i on item j, and X is a\nk \u00d7 m matrix with each column as an estimate of the value of\nthe random variable X (user's ratings in the k-dimensional\nrating space) for one user. Typically, the matrix Y is very\nsparse.\nTo remedy this, Canny [52] proposed a sparse factor\nanalysis, which replaces missing elements with default voting\nvalues (the average of some nonmissing elements, either\nthe average by columns, or by rows, or by all), and uses\nthe regression model as the initialization for Expectation\nMaximization (EM) [85] iterations. According to Canny\n[52], the sparse factor analysis has better scalability than Pearson\ncorrelation-based CF and Personality Diagnosis (PD), a\nrepresentative hybrid CF algorithm [86], and better accuracy\nthan singular value decomposition (SVD) [23]. Sparse factor\nanalysis also protects user privacy, as it supports computation\non encrypted user data [52].\nVucetic and Obradovic [87] proposed a regressionbased\napproach to CF tasks on numerical ratings data that\nsearches for similarities between items, builds a collection\nof simple linear models, and combines them efficiently to\nprovide rating predictions for an active user. They used\nordinary least squares to estimate the parameters of the linear\nregression function. Their experimental results show the\napproach has good performance in addressing the sparsity,\nprediction latency and numerical prediction problems of\nCF tasks. Lemire and Maclachlan [88] proposed slope one\nalgorithms to make faster CF prediction than memory-based\nCF algorithms.</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">4.4. MDP-Based CF Algorithms. Instead of viewing the\nrecommendation process as a prediction problem, Shani\net al. [14] views it as a sequential optimization problem\nand uses a Markov decision processes (MDPs) model [89] for\nrecommender systems.\nAn MDP is a model for sequential stochastic decision\nproblems, which is often used in applications where an agent\nis influencing its surrounding environment through actions.\nAn MDP can be defined as a four-tuple: S, A, R, Pr , where\nS is a set of states, A is a set of actions, R is a real-valued\nreward function for each state/action pair, and Pr is the\ntransition probability between every pair of states given each\naction.\nAn optimal solution to the MDP is to maximize the\nfunction of its reward stream. By starting with an initial\npolicy \u03c00(s) = arg maxa\u2208AR(s, a), computing the reward\nvalue function Vi(s) based on the previous policy, and\nupdating the policy with the new value function at each step,\nthe iterations will converge to an optimal policy [90, 91].\nIn Shani et al. [14], the states of the MDP for the\nCF system are k tuples of items, with some null values\ncorresponding to missing items; the actions of the MDP\ncorrespond to a recommendation of an item; and the rewards\nin the MDP correspond to the utility of selling an item,\nfor example, the net profit. The state following each recommendation\nis the user's response to that recommendation,\nsuch as taking the recommended item, taking the nonrecommended\nitem, or selecting nothing. To handle the large\naction space, it is assumed that the probability that a user\nbuys an item depends on his current state, item, and whether\nor not the item is recommended, but does not depend on the\nidentity of the other recommended items.\nWorking on an Israeli online bookstore, Mitos, the\ndeployed MDP-recommender system produced a much\nhigher profit than the system without using the recommender.\nAlso, the MDP CF model performs much better\nthan the simpler Markov chain (MC) model, which is simply\nan MDP without actions [14].\nThe MDP-Based CF model in Shani et al. [14] can\nbe viewed as approximating a partial observable MDP\n(POMDP) by using a finite rather than unbounded window\nof past history to define the current state. As the\ncomputational and representational complexity of POMDPs\nis high, appropriate approaches to tackling these problems\nmust be developed, which are generally classified into three\nbroad strategies: value function approximation [92], policy\nbased optimization [84, 93], and stochastic sampling [94].\nThe application of these strategies to CF tasks may be an\ninteresting direction of future research.\n4.5. Latent Semantic CF Models. A Latent semantic CF\ntechnique relies on a statistical modeling technique that\nintroduces latent class variables in a mixture model setting\nto discover user communities and prototypical interest\nprofiles. Conceptionally, it decomposes user preferences\nusing overlapping user communities. The main advantages\nof this technique over standard memory-based methods are\nits higher accuracy and scalability [7, 95].</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">The aspect model, proposed by Hofmann and Puzicha\n[83], is a probabilistic latent-space model, which models\nindividual ratings as a convex combination of rating factors.\nThe latent class variable is associated with each observed pair\nof {user, item}, with the assumption that users and items are\nindependent from each other given the latent class variable.\nThe performance of the aspect model is much better than the\nclustering model working on the EachMovie dataset [96].\nA multinomial model is a simple probabilistic model for\ncategorical data [9, 97] that assumes there is only one type\nof user. A multinomial mixture model assumes that there are\nmultiple types of users underlying all profiles, and that the\nrating variables are independent with each other and with\nthe user's identity given the user's type [98]. A user rating\nprofile (URP) model [97] combines the intuitive appeal of the\nmultinomial mixture model and aspect model [83], with the\nhigh-level generative semantics of Latent Dirichlet Allocation\n(LDA, a generative probabilistic model, in which each item\nis modeled as a finite mixture over an underlying set of\nusers) [99]. URP performs better than the aspect model and\nmultinomial mixtures models for CF tasks.\n4.6. Other Model-Based CF Techniques. For applications in\nwhich ordering is more desirable than classifying, Cohen\net al. [100] investigated a two-stage order learning CF\napproach to learning to order. In that approach, one first\nlearns a preference function by conventional means, and then\norders a new set of instances by finding the total ordering that\nbest approximates the preference function, which returns a\nconfidence value reflecting how likely that one is preferred\nto another. As the problem of finding the total ordering is\nNP-complete, a greedy-order algorithm is used to obtain\nan approximately optimal ordering function. Working on\nEachMovie [96], this order learning CF approach performs\nbetter than a nearest neighbor CF algorithm and a linear\nregression algorithm.\nAssociation rule based CF algorithms are more often\nused for top-N recommendation tasks than prediction ones.\nSarwar et al. [27] describes their approach to using a\ntraditional association rule mining algorithm to find rules\nfor developing top-N recommender systems. They find the\ntop-N items by simply choosing all the rules that meet\nthe thresholds for support and confidence values, sorting\nitems according to the confidence of the rules so that items\npredicted by the rules that have a higher confidence value\nare ranked higher, and finally selecting the first N highest\nranked items as the recommended set [27]. Fu et al. [101]\ndevelop a system to recommend web pages by using an\na priori algorithm to mine association rules over users'\nnavigation histories. Leung et al. proposed a collaborative\nfiltering framework using fuzzy association rules and multilevel\nsimilarity [102].\nOther model-based CF techniques include a maximum\nentropy approach, which clusters the data first, and then in a\ngiven cluster uses maximum entropy as an objective function\nto form a conditional maximal entropy model to make\npredictions [17]. A dependency network is a graphical model\nfor probabilistic relationships, whose graph is potentially</zone>\n  <zone label=\"GEN_OTHER\">11</zone>\n  <zone label=\"BODY_CONTENT\">cyclic. The probability component of a dependency network\nis a set of conditional distributions, one for each node\ngiven its parents. Although less accurate than Bayesian\nbelief nets, dependency networks are faster in generating\npredictions and require less time and memory to learn\n[75]. Decision tree CF models treat collaborative filtering\nas a classification task and use decision tree as the classifier\n[103]. Horting is a graph-based technique in which\nnodes are users and edges between nodes are degrees\nof similarity between users [104]. Multiple multiplicative\nfactor models (MMFs) are a class of causal, discrete latent\nvariable models combining factor distributions multiplicatively\nand are able to readily accommodate missing data\n[105]. Probabilistic principal components analysis (pPCA)\n[52, 106] determines the principal axes of a set of observed\ndata vectors through maximum-likelihood estimation of\nparameters in a latent variable model closely related to\nfactor analysis. Matrix factorization based CF algorithms\nhave been proven to be effective to address the scalability\nand sparsity challenges of CF tasks [33, 34, 107]. Wang\net al. showed how the development of collaborative filtering\ncan gain benefits from information retrieval theories and\nmodels, and proposed probabilistic relevance CF models\n[108, 109].</zone>\n  <zone label=\"BODY_HEADING\">5. Hybrid Collaborative Filtering Techniques</zone>\n  <zone label=\"BODY_CONTENT\">Hybrid CF systems combine CF with other recommendation\ntechniques (typically with content-based systems) to make\npredictions or recommendations.\nContent-based recommender systems make recommendations\nby analyzing the content of textual information,\nsuch as documents, URLs, news messages, web logs, item\ndescriptions, and profiles about users' tastes, preferences, and\nneeds, and finding regularities in the content [110]. Many\nelements contribute to the importance of the textual content,\nsuch as observed browsing features of the words or pages\n(e.g., term frequency and inverse document frequency), and\nsimilarity between items a user liked in the past [111]. A\ncontent-based recommender then uses heuristic methods or\nclassification algorithms to make recommendations [112].\nContent-based techniques have the start-up problem, in\nwhich they must have enough information to build a reliable\nclassifier. Also, they are limited by the features explicitly associated\nwith the objects they recommend (sometimes these\nfeatures are hard to extract), while collaborative filtering\ncan make recommendations without any descriptive data.\nAlso, content-based techniques have the overspecialization\nproblem, that is, they can only recommend items that score\nhighly against a user's profile or his/her rating history [21,\n113].\nOther recommender systems include demographic-based\nrecommender systems, which use user profile information\nsuch as gender, postcode, occupation, and so forth [114];\nutility-based recommender systems and knowledge-based recommender\nsystems, both of which require knowledge about\nhow a particular object satisfies the user needs [115, 116].\nWe will not discuss these systems in detail in this work.\nI1\n2\n2\n3\n3\n1\nI1\n2\n3\n3\n3\n1\nF\nM\nM\nF\nM\nI2\n3\n2\n1\n3\n2\nI2\n3\n4\n3\n3\n3</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">Table 5: Content-boosted CF and its variations (a) content data\nand originally sparse rating data (b) pseudorating data filled by\ncontent predictor (c) predictions from (weighted) Pearson CF on\nthe pseudo rating data.\nAge Sex\nU1 32\nU2 27\nU3 24\nU4 50\nU5 28\n(a)\nContent information\nCareer\nwriter\nstudent\nzip\n22904\n10022 2\nengineer 60402\nother\n60804\neducator 85251 1\n(b)\nPseudo rating data\nRating matrix\nI1 I2 I3 I4 I5\n4\n1\n3\n4\n3\n3\n3\n3\nI3\n4\n2\n2\n3\n1\nI3\n4\n4\n3\n3\n4\n(c)\nI4\n2\n2\n3\n3\n2\nI4\n3\n3\n4\n3\n1\nPearson-CF prediction\nI5\n3\n3\n3\n3\n2\nI5\n2\n2\n3\n3\n2</zone>\n  <zone label=\"GEN_OTHER\">12</zone>\n  <zone label=\"BODY_CONTENT\">Hoping to avoid limitations of either recommender\nsystem and improve recommendation performance, hybrid\nCF recommenders are combined by adding content-based\ncharacteristics to CF models, adding CF characteristics to\ncontent-based models, combining CF with content-based or\nother systems, or combining different CF algorithms [21,\n117].\n5.1. Hybrid Recommenders Incorporating CF and ContentBased\nFeatures. The content-boosted CF algorithm uses na\u00a8\u0131ve\nBayes as the content classifier, it then fills in the missing\nvalues of the rating matrix with the predictions of the\ncontent predictor to form a pseudo rating matrix, in which\nobserved ratings are kept untouched and missing ratings\nare replaced by the predictions of a content predictor.\nIt then makes predictions over the resulting pseudo ratings\nmatrix using a weighted Pearson correlation-based CF\nalgorithm, which gives a higher weight for the item that\nmore users rated, and gives a higher weight for the active\nuser [16] (see an illustration in Table 5). The contentboosted\nCF recommender has improved prediction performance\nover some pure content-based recommenders and\nsome pure memory-based CF algorithms. It also overcomes\nthe cold start problem and tackles the sparsity problem\nof CF tasks. Working on reasonably-sized subsets instead\nof the original rating data, Greinemr et al. used TANELR\n[31] as the content-predictor and directly applied the\nPearson correlation-based CF instead of a weighted one\non the pseudo rating matrix to make predictions, and\nthey achieved improved CF performance in terms of MAE\n[118].\nAnsari et al. [8] propose a Bayesian preference model\nthat statistically integrates several types of information useful\nfor making recommendations, such as user preferences, user\nand item features, and expert evaluations. They use Markov\nchain Monte Carlo (MCMC) methods [119] for samplingbased\ninference, which involve sampling parameter estimation\nfrom the full conditional distribution of parameters.\nThey achieved better performance than pure collaborative\nfiltering.\nThe recommender Fab, proposed by Balabanovic\u00b4 and\nShoham [117], maintains user profiles of interest in web\npages using content-based techniques, and uses CF techniques\nto identify profiles with similar tastes. It can then\nrecommend documents across user profiles. Sarwar et al.\n[120] implemented a set of knowledge-based \u201cfilterbots\u201d\nas artificial users using certain criteria. A straightforward\nexample of a filterbot is a genrebot, which bases its opinion\nsolely on the genre of the item, for example, a \u201cjazzbot\u201d\nwould give a full mark to a CD simply because it is in the jazz\ncategory, while it would give a low score to any other CD in\nthe database. Mooney and Roy [121] use the prediction from\nthe CF system as the input to a content-based recommender.\nCondiff et al. [113] propose a Bayesian mixed-effects model\nthat integrates user ratings, user, and item features in a single\nunified framework. The CF system Ripper, proposed by Basu\net al. [71], uses both user ratings and contents features to\nproduce recommendations.\n5.2. Hybrid Recommenders Combining CF and Other Recommender\nSystems. A weighted hybrid recommender combines\ndifferent recommendation techniques by their weights,\nwhich are computed from the results of all of the available\nrecommendation techniques present in the system [115].\nThe combination can be linear, the weights can be adjustable\n[46], and weighted majority voting [110, 122] or weighted\naverage voting [118] can be used. For example, the PTango\nsystem [46] initially gives CF and content-based\nrecommenders equal weight, but gradually adjusts the\nweighting as predictions about user ratings are confirmed or\ndisconfirmed. The strategy of the P-Tango system is similar\nto boosting [123].\nA switching hybrid recommender switches between recommendation\ntechniques using some criteria, such as confidence\nlevels for the recommendation techniques. When the\nCF system cannot make a recommendation with sufficient\nconfidence, then another recommender system such as a\ncontent-based system is attempted. Switching hybrid recommenders\nalso introduce the complexity of parameterization\nfor the switching criteria [115].\nOther hybrid recommenders in this category include\nmixed hybrid recommenders [124], cascade hybrid recommenders\n[115], meta-level recommenders [110, 115, 117, 125],\nand so forth.</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">Many papers empirically compared the performance of\nhybrid recommenders with the pure CF and content-based\nmethods and found that hybrid recommenders may make\nmore accurate recommendations, especially for the new user\nand new item situations where a regular CF algorithm cannot\nmake satisfactory recommendations. However, hybrid recommenders\nrely on external information that is usually not\navailable, and they generally have increased complexity of\nimplementation [110, 115, 126].\n5.3. Hybrid Recommenders Combining CF Algorithms. The\ntwo major classes of CF approaches, memory-based and\nmodel-based CF approaches, can be combined to form\nhybrid CF approaches. The recommendation performances\nof these algorithms are generally better than some pure\nmemory-based CF algorithms and model-based CF algorithms\n[22, 86].\nProbabilistic memory-based collaborative filtering (PMCF)\ncombines memory-based and model-based techniques [22].\nThey use a mixture model built on the basis of a set\nof stored user profiles and use the posterior distribution\nof user ratings to make predictions. To address the new\nuser problem, an active learning extension to the PMCF\nsystem can be used to actively query a user for additional\ninformation when insufficient information is available. To\nreduce the computation time, PMCF selects a small subset\ncalled profile space from the entire database of user ratings\nand gives predictions from the small profile space instead\nof the whole database. PMCF has better accuracy than the\nPearson correlation-based CF and the model-based CF using\nna\u00a8\u0131ve Bayes.\nPersonality diagnosis (PD) is a representative hybrid CF\napproach that combines memory-based and model-based CF\nalgorithms and retains some advantages of both algorithms\n[86]. In PD, the active user is assumingly generated by\nchoosing one of the other users uniformly at random and\nadding Gaussian noise to his or her ratings. Given the active\nuser's known ratings, we can calculate the probability that\nhe or she is the same \u201cpersonality type\u201d as other users, and\nthe probability he or she will like the new items. PD can also\nbe regarded as a clustering method with exactly one user per\ncluster. Working on EachMovie [96] and CiteSeer [127], PD\nmakes better predictions than Pearson correlation-based and\nvector similarity-based CF algorithms and the two modelbased\nalgorithms, Bayesian clustering and Bayesian network,\ninvestigated by Breese et al. [9].\nAs an ensemble classifier is able to give more accurate\nprediction than a member classifier, a hybrid CF system that\ncombines different CF algorithms using an ensemble scheme\nwill also be helpful to improve predictive performance of CF\ntasks [118].</zone>\n  <zone label=\"BODY_HEADING\">6. Evaluation Metrics</zone>\n  <zone label=\"BODY_CONTENT\">The quality of a recommender system can be decided on\nthe result of evaluation. The type of metrics used depends\non the type of CF applications. According to Herlocker\net al. [60], metrics evaluating recommendation systems can\nbe broadly classified into the following broad categories:\npredictive accuracy metrics, such as Mean Absolute Error\n(MAE) and its variations; classification accuracy metrics, such\nas precision, recall, F1-measure, and ROC sensitivity; rank\naccuracy metrics, such as Pearson's product-moment correlation,\nKendall's Tau, Mean Average Precision (MAP), half-life\nutility [9], and normalized distance-based performance metric\n(NDPM) [128].\nWe only introduce the commonly-used CF metrics\nMAE, NMAE, RMSE, and ROC sensitivity here. For other\nCF performance metrics of recommendation quality, see\n[60]. There are other evaluations of recommender systems\nincluding usability evaluation [129] and so forth.\n6.1. Mean Absolute Error (MAE) and Normalized Mean\nAbsolute Error (NMAE). Instead of classification accuracy\nor classification error, the most widely used metric in CF\nresearch literature is Mean Absolute Error (MAE) [3, 60],\nwhich computes the average of the absolute difference\nbetween the predictions and true ratings</zone>\n  <zone label=\"GEN_OTHER\">13</zone>\n  <zone label=\"BODY_CONTENT\">(14)\n(15)\n(16)\nMAE\nNMAE = rmax \u2212 rmin ,\nwhere rmax and rmin are the upper and lower bounds of the\nratings.\n6.2. Root Mean Squared Error (RMSE). Root Mean Squared\nError (RMSE) is becoming popular partly because it is\nthe Netflix prize [20] metric for movie recommendation\nperformance:\nMAE =\n{i,j} pi,j \u2212 ri,j\nn\n,\nwhere n is the total number of ratings over all users, pi,j is\nthe predicted rating for user i on item j, and ri,j is the actual\nrating. The lower the MAE, the better the prediction.\nDifferent recommender systems may use different\nnumerical rating scales. Normalized Mean Absolute Error\n(N MAE) normalizes MAE to express errors as percentages\nof full scale [3]:\nRMSE =\n1\nn {i,j}\n2\npi,j \u2212 ri,j ,\nwhere n is the total number of ratings over all users, pi,j\nis the predicted rating for user i on item j, and ri,j is the\nactual rating again. RMSE amplifies the contributions of the\nabsolute errors between the predictions and the true values.\nAlthough accuracy metrics have greatly helped the field\nof recommender systems, the recommendations that are\nmost accurate are sometimes not the ones that are most\nuseful to users, for example, users might prefer to be\nrecommended with items that are unfamiliar with them,\nrather than the old favorites they do not likely want again\n[130]. We therefore need to explore other evaluation metrics.</zone>\n  <zone label=\"GEN_OTHER\">14</zone>\n  <zone label=\"BODY_HEADING\">Actual\nPositive\nNegative</zone>\n  <zone label=\"BODY_CONTENT\">Table 6: Confusion matrix.\nPredicted\nPositive\nTruePositive\nFalsePositive\nNegative\nFalseNegative\nTureNegative\n6.3. ROC Sensitivity. An ROC (Receiver Operating Characteristic)\ncurve is a two-dimensional depiction of classifier\nperformance, on which TPR (true positive rate) is plotted\non the Y -axis and FPR (false positive rate) is plotted on\nthe X-axis. For the confusion matrix in Table 6, we have\nTPR = TruePositive/(TotalPositive), and FPR = FalsePositive/(TotalNegative).\nBy tuning a threshold value, all the items\nranked above it are deemed observed by the user, and below\nunobserved, thus the system will get different prediction\nvalues for different threshold values to draw the ROC curve\nof {FPR, TPR} points [60].\nGenerally, if one ROC curve is consistently dominant\nover another, the system represented by the former curve has\nbetter prediction performance. But in actual situations, ROC\ncurves may intersect with each other.\nVariations of the ROC metric include GROC (global\nROC) and CROC (customer ROC) [29].\nROC sensitivity is a measure of the diagnostic power of\na CF system. Operationally, it is given by the Area Under the\nROC Curve (AUC). The calculation of AUC can be the actual\narea under the ROC curve for binary class problems. We can\nalso use the strategy from [131] to estimate AUC:\nAUC =\nS0 \u2212 n0(n0 + 1)/2\nn0n1\n(17)\nwhere n0 and n1 are the numbers of negative and positive\nexamples respectively, and S0 = \u03a3ri , where ri is the rank of ith\npositive example in the ranked list. From the above equation,\nthe AUC is essentially a measure of the quality of a ranking.\nFor multiclass problems, we can use estimated AUC,\nwhich can be the weighted average of the AUCs obtained by\ntaking each class as the reference class in turn (i.e., making\nit class 0 and all other classes class 1). The weight of a class's\nAUC is the class's frequency in the data [129, 132].\nThe performance of the recommender system with a\nbigger AUC value is better.</zone>\n  <zone label=\"BODY_HEADING\">7. Conclusions</zone>\n  <zone label=\"BODY_CONTENT\">Collaborative filtering (CF) is one of the most successful recommender\ntechniques. Broadly, there are memory-based CF\ntechniques such as the neighborhood-based CF algorithm;\nmodel-based CF techniques such as Bayesian belief nets CF\nalgorithms, clustering CF algorithms, and MDP-based CF\nalgorithms; and hybrid CF techniques such as the contentboosted\nCF algorithm and Personality diagnosis.\nAs a representative memory-based CF technique, neighborhood-based\nCF computes similarity between users or\nitems, and then use the weighted sum of ratings or simple\nweighted average to make predictions based on the similarity\nvalues. Pearson correlation and vector cosine similarity are</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">commonly used similarity calculations, which are usually\nconducted between co-rated items by a certain user or\nboth users that have co-rated a certain item. To make topN\nrecommendations, neighborhood-based methods can be\nused according to the similarity values. Memory-based CF\nalgorithms are easy to implement and have good performances\nfor dense datasets. Shortcomings of memory-based\nCF algorithms include their dependence on user ratings,\ndecreased performance when data are sparse, new users and\nitems problems, and limited scalability for large datasets, and\nso forth [11, 42, 133]. Memory-based CF on imputed rating\ndata and on dimensionality-reduced rating data will produce\nmore accurate predictions than on the original sparse rating\ndata [24, 25, 37].\nModel-based CF techniques need to train algorithmic\nmodels, such as Bayesian belief nets, clustering techniques, or\nMDP-based ones to make predictions for CF tasks. Advanced\nBayesian belief nets CF algorithms with the ability to deal\nwith missing data are found to have better performance than\nsimple Bayesian CF models and Pearson correlation-based\nalgorithms [11]. Clustering CF algorithms make recommendations\nwithin small clusters rather than the whole dataset,\nand achieve better scalability. An MDP-based CF algorithm\nincorporates the users' action of taking the recommendation\nor not into the model, and the optimal solution to the\nMDP is to maximize the function of its reward stream. The\nMDP-based CF algorithm brings profits to the customized\nsystem deploying it. There are downsides of model-based CF\ntechniques, for example, they may not be practical when the\ndata are extremely sparse, the solutions using dimensionality\nreduction or transformation of multiclass data into binary\nones may decrease their recommendation performance, the\nmodel-building expense may be high, and there is a tradeoff\nbetween prediction performance and scalability for many\nalgorithms.\nMost hybrid CF techniques combine CF methods\nwith content-based techniques or other recommender systems\nto alleviate shortcomings of either system and to\nimprove prediction and recommendation performance.\nBesides improved performance, hybrid CF techniques rely\non external content information that is usually not available,\nand they generally have increased complexity.\nIt is always desirable to design a CF approach that is\neasy to implement, takes few resources, produces accurate\npredictions and recommendations, and overcomes all kinds\nof challenges presented by real-world CF applications, such\nas data sparsity, scalability, synonymy, privacy protection,\nand so forth. Although there is no cure-all solution available\nyet, people are working out solutions for each of the\nproblems. To alleviate the sparsity problem of CF tasks,\nmissing-data algorithms such as TAN-ELR [31], imputation\ntechniques such as Bayesian multiple imputation [68], and\ndimensionality reduction techniques such as SVD [23]\nand matrix factorization [107] can be used. Clustering CF\nalgorithms and other approaches such as an incrementalSVD\nCF algorithm [38] are found promising in dealing with\nthe scalability problem. Latent semantic indexing (LSI) is\nhelpful to handle the synonymy problem. And sparse factor\nanalysis is found helpful to protect user privacy [52].</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"BODY_CONTENT\">Besides addressing the above challenges, future CF\ntechniques should also be able to make accurate predictions\nin the presence of shilling attacks and noisy data, and be\neffectively applied in fast-growing mobile applications as\nwell.\nThere are many evaluation metrics for CF techniques.\nThe most commonly used metric for prediction accuracy\ninclude mean absolute error (MAE), recall and precision, and\nROC sensitivity. Because artificial data are usually not reliable\ndue to the characteristics of CF tasks, real-world datasets\nfrom live experiments are more desirable for CF research.</zone>\n  <zone label=\"BODY_HEADING\">Acknowledgment</zone>\n  <zone label=\"BODY_CONTENT\">The authors are grateful to Drs. Miroslav Kubat and Moiez\nA. Tapia for their help during the early stage of this paper\nand also to Drs. Xingquan Zhu, Russ Greiner, Andres Folleco,\nand Amri Napolitano for their comments. Their thanks\nalso go to Dr. Jun Hong and other editors/reviewers of the\nAAI Journal for their work as well as to many anonymous\nreviewers for their comments. This work was supported\nby the Data Mining and Machine Learning and Empirical\nSoftware Engineering and Laboratories at Florida Atlantic\nUniversity.</zone>\n  <zone label=\"GEN_REFERENCES\">References\n[1] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry, \u201cUsing\ncollaborative filtering to weave an information tapestry,\u201d\nCommunications of ACM, vol. 35, no. 12, pp. 61-70, 1992.\n[2] P. Resnick and H. R. Varian, \u201cRecommender systems,\u201d\nCommunications of the ACM, vol. 40, no. 3, pp. 56-58, 1997.\n[3] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins, \u201cEigentaste:\na constant time collaborative filtering algorithm,\u201d\nInformation Retrieval, vol. 4, no. 2, pp. 133-151, 2001.\n[4] B. N. Miller, J. A. Konstan, and J. Riedl, \u201cPocketLens: toward\na personal recommender system,\u201d ACM Transactions on\nInformation Systems, vol. 22, no. 3, pp. 437-476, 2004.\n[5] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl,\n\u201cGrouplens: an open architecture for collaborative filtering of\nnetnews,\u201d in Proceedings of the ACM Conference on Computer\nSupported Cooperative Work, pp. 175-186, New York, NY,\nUSA, 1994.\n[6] G. Linden, B. Smith, and J. York, \u201cAmazon.com recommendations:\nitem-to-item collaborative filtering,\u201d IEEE Internet\nComputing, vol. 7, no. 1, pp. 76-80, 2003.\n[7] T. Hofmann, \u201cLatent semantic models for collaborative\nfiltering,\u201d ACM Transactions on Information Systems, vol. 22,\nno. 1, pp. 89-115, 2004.\n[8] A. Ansari, S. Essegaier, and R. Kohli, \u201cInternet recommendation\nsystems,\u201d Journal of Marketing Research, vol. 37, no. 3,\npp. 363-375, 2000.\n[9] J. Breese, D. Heckerman, and C. Kadie, \u201cEmpirical analysis\nof predictive algorithms for collaborative filtering,\u201d in Proceedings\nof the 14th Conference on Uncertainty in Artificial\nIntelligence (UAI '98), 1998.\n[10] K. Miyahara and M. J. Pazzani, \u201cCollaborative filtering with\nthe simple Bayesian classifier,\u201d in Proceedings of the 6th Pacific\nRim International Conference on Artificial Intelligence, pp.\n679-689, 2000.</zone>\n  <zone label=\"GEN_OTHER\">15</zone>\n  <zone label=\"GEN_REFERENCES\">[11] X. Su and T. M. Khoshgoftaar, \u201cCollaborative filtering for\nmulti-class data using belief nets algorithms,\u201d in Proceedings\nof the International Conference on Tools with Artificial Intelligence\n(ICTAI '06), pp. 497-504, 2006.\n[12] L. H. Ungar and D. P. Foster, \u201cClustering methods for\ncollaborative filtering,\u201d in Proceedings of the Workshopon\nRecommendation Systems, AAAI Press, 1998.\n[13] S. H. S. Chee, J. Han, and K. Wang, \u201cRecTree: an efficient\ncollaborative filtering method,\u201d in Proceedings of the 3rd\nInternational Conference on Data Warehousing and Knowledge\nDiscovery, pp. 141-151, 2001.\n[14] G. Shani, D. Heckerman, and R. I. Brafman, \u201cAn MDP-based\nrecommender system,\u201d Journal of Machine Learning Research,\nvol. 6, pp. 1265-1295, 2005.\n[15] L. Si and R. Jin, \u201cFlexible mixture model for collaborative\nfiltering,\u201d in Proceedings of the 20th International Conference\non Machine Learning (ICML '03), vol. 2, pp. 704-711,\nWashington, DC, USA, August 2003.\n[16] P. Melville, R. J. Mooney, and R. Nagarajan, \u201cContentboosted\ncollaborative filtering for improved recommendations,\u201d\nin Proceedings of the 18th National Conference on\nArtificial Intelligence (AAAI '02), pp. 187-192, Edmonton,\nCanada, 2002.\n[17] D. Y. Pavlov and D. M. Pennock, \u201cA maximum entropy\napproach to collaborative filtering in dynamic, sparse, highdimensional\ndomains,\u201d in Advances in Neural Information\nProcessing Systems, pp. 1441-1448, MIT Press, Cambridge,\nMass, USA, 2002.\n[18] MovieLens data, http://www.grouplens.org/.\n[19] Jester data, http://shadow.ieor.berkeley.edu/humor/.\n[20] Netflix prize, http://www.netflixprize.com/.\n[21] G. Adomavicius and A. Tuzhilin, \u201cToward the next generation\nof recommender systems: a survey of the state-of-theart\nand possible extensions,\u201d IEEE Transactions on Knowledge\nand Data Engineering, vol. 17, no. 6, pp. 734-749, 2005.\n[22] K. Yu, A. Schwaighofer, V. Tresp, X. Xu, and H.-P. Kriegel,\n\u201cProbabilistic memory-based collaborative filtering,\u201d IEEE\nTransactions on Knowledge and Data Engineering, vol. 16, no.\n1, pp. 56-69, 2004.\n[23] D. Billsus and M. Pazzani, \u201cLearning collaborative information\nfilters,\u201d in Proceedings of the 15th International\nConference on Machine Learning (ICML '98), 1998.\n[24] T. Landauer, M. Littman, and Bell Communications\nResearch (Bellcore), \u201cComputerized cross-language document\nretrieval using latent semantic indexing,\u201d US patent no.\n5301109, April 1994.\n[25] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer,\nand R. Harshman, \u201cIndexing by latent semantic analysis,\u201d\nJournal of the American Society for Information Science, vol.\n41, no. 6, pp. 391-407, 1990.\n[26] K. Pearson, \u201cOn lines and planes of closest fit to systems of\npoints in space,\u201d Philosophical Magazine, vol. 2, pp. 559-572,\n1901.\n[27] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl, \u201cAnalysis\nof recommendation algorithms for E-commerce,\u201d in Proceedings\nof the ACM E-Commerce, pp. 158-167, Minneapolis,\nMinn, USA, 2000.\n[28] C.-N. Ziegler, G. Lausen, and L. Schmidt-Thieme, \u201cTaxonomy-driven\ncomputation of product recommendations,\u201d in\nProceedings of the 13th International Conference on Information\nand Knowledge Management (CIKM '04), pp. 406-415,\nWashington, DC, USA, November 2004.</zone>\n  <zone label=\"GEN_OTHER\">16</zone>\n  <zone label=\"GEN_REFERENCES\">[29] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. Pennock,\n\u201cMethods and metrics for cold-start recommendations,\u201d in\nProceedings of the 25th Annual International ACM SIGIR\nConference on Research and Development in Information\nRetrieval (SIGIR '02), 2002.\n[30] B. M. Kim and Q. Li, \u201cProbabilistic model estimation\nfor collaborative filtering based on items attributes,\u201d in\nProceedings of the IEEE/WIC/ACM International Conference\non Web Intelligence (WI '04), pp. 185-191, Beijing, China,\nSeptember 2004.\n[31] R. Greinemr, X. Su, B. Shen, and W. Zhou, \u201cStructural\nextension to logistic regression: discriminative parameter\nlearning of belief net classifiers,\u201d Machine Learning, vol. 59,\nno. 3, pp. 297-322, 2005.\n[32] Z. Huang, H. Chen, and D. Zeng, \u201cApplying associative\nretrieval techniques to alleviate the sparsity problem in\ncollaborative filtering,\u201d ACM Transactions on Information\nSystems, vol. 22, no. 1, pp. 116-142, 2004.\n[33] N. Srebro, J. D. M. Rennie, and T. Jaakkola, \u201cMaximummargin\nmatrix factorization,\u201d in Advances in Neural Information\nProcessing Systems, vol. 17, pp. 1329-1336, 2005.\n[34] J. D. M. Rennie and N. Srebro, \u201cFast maximum margin\nmatrix factorization for collaborative prediction,\u201d in Proceedings\nof the 22nd International Conference on Machine Learning\n(ICML '05), Bonn, Germany, August 2005.\n[35] D. DeCoste, \u201cCollaborative prediction using ensembles of\nmaximum margin matrix factorizations,\u201d in Proceedings\nof the 23rd International Conference on Machine Learning\n(ICML '06), pp. 249-256, Pittsburgh, Pa, USA, June 2006.\n[36] H. Noh, M. Kwak, and I. Han, \u201cmproving the prediction\nperformance of customer behavior through multiple imputation,\u201d\nIntelligent Data Analysis, vol. 8, no. 6, pp. 563-577,\n2004.\n[37] X. Su, T. M. Khoshgoftaar, and R. Greiner, \u201cA mixture\nimputation-boosted collaborative filter,\u201d in Proceedings of\nthe 21th International Florida Artificial Intelligence Research\nSociety Conference (FLAIRS '08), pp. 312-317, Coconut\nGrove, Fla, USA, May 2008.\n[38] B. M. Sarwar, G. Karypis, J. Konstan, and J. Riedl, \u201cIncremental\nSVD-based algorithms for highly scaleable recommender\nsystems,\u201d in Proceedings of the 5th International Conference on\nComputer and Information Technology (ICCIT '02), 2002.\n[39] M. W. Berry, S. T. Dumais, and G. W. O'Brien, \u201cUsing linear\nalgebra for intelligent information retrieval,\u201d SIAM Review,\nvol. 37, no. 4, pp. 573-595, 1995.\n[40] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl, \u201cItembased\ncollaborative filtering recommendation algorithms,\u201d\nin Proceedings of the 10th International Conference on World\nWide Web (WWW '01), pp. 285-295, May 2001.\n[41] K. Miyahara and M. J. Pazzani, \u201cImprovement of collaborative\nfiltering with the simple Bayesian classifier,\u201d Information\nProcessing Society of Japan, vol. 43, no. 11, 2002.\n[42] M. O'Connor and J. Herlocker, \u201cClustering items for collaborative\nfiltering,\u201d in Proceedings of the ACM SIGIR Workshop\non Recommender Systems (SIGIR '99), 1999.\n[43] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl,\n\u201cRecommender systems for large-scale E-commerce: scalable\nneighborhood formation using clustering,\u201d in Proceedings of\nthe 5th International Conference on Computer and Information\nTechnology (ICCIT '02), December 2002.\n[44] G.-R. Xue, C. Lin, Q. Yang, et al., \u201cScalable collaborative\nfiltering using cluster-based smoothing,\u201d in Proceedings of the\nACM SIGIR Conference, pp. 114-121, Salvador, Brazil, 2005.</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"GEN_REFERENCES\">[45] S. K. Jones, \u201cA statistical interpretation of term specificity and\nits applications in retrieval,\u201d Journal of Documentation, vol.\n28, no. 1, pp. 11-21, 1972.\n[46] M. Claypool, A. Gokhale, T. Miranda, et al., \u201cCombining\ncontent-based and collaborative filters in an online newspaper,\u201d\nin Proceedings of the SIGIR Workshop on Recommender\nSystems: Algorithms and Evaluation, Berkeley, Calif, USA,\n1999.\n[47] J. McCrae, A. Piatek, and A. Langley, \u201cCollaborative Filtering,\u201d\nhttp://www.imperialviolet.org/.\n[48] S. K. Lam and J. Riedl, \u201cShilling recommender systems for\nfun and profit,\u201d in Proceedings of the 13th International World\nWide Web Conference (WWW '04), pp. 393-402, New York,\nNY, USA, May 2004.\n[49] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams,\n\u201cEffective attack models for shilling item-based collaborative\nfiltering systems,\u201d in Proceedings of the WebKDD Workshop,\nAugust 2005.\n[50] M. O'Mahony, N. Hurley, N. Kushmerick, and G. Silvestre,\n\u201cCollaborative recommendation: a robustness analysis,\u201d\nACM Transactions on Internet Technology, vol. 4, no. 4, pp.\n344-377, 2004.\n[51] R. Bell and Y. Koren, \u201cImproved neighborhood-based collaborative\nfiltering,\u201d in Proceedings of KDD Cup and Workshop,\n2007.\n[52] J. Canny, \u201cCollaborative filtering with privacy via factor\nanalysis,\u201d in Proceedings of the 25th Annual International ACM\nSIGIR Conference on Research and Development in Information\nRetrieval, pp. 238-245, Tampere, Finland, August 2002.\n[53] K. Yu, X. Xu, J. Tao, M. Ester, and H.-P. Kriegel, \u201cInstance\nselection techniques for memory-based collaborative filtering,\u201d\nin Proceedings of the SIAM International Conference on\nData Mining (SDM '02), April 2002.\n[54] G. Shafer, A Mathematical Theory of Evidence, Princeton\nUniversity Press, Princeton, NJ, USA, 1976.\n[55] D. Cai, M. F. McTear, and S. I. McClean, \u201cKnowledge\ndiscovery in distributed databases using evidence theory,\u201d\nInternational Journal of Intelligent Systems, vol. 15, no. 8, pp.\n745-761, 2000.\n[56] T. M. Khoshgoftaar and J. V. Hulse, \u201cMultiple imputation\nof software measurement data: a case study,\u201d in Proceedings\nof the International Conference on Software Engineering and\nKnowledge Engineering (SEKE '06), 2006.\n[57] Y. Koren, \u201cTutorial on recent progress in collaborative\nfiltering,\u201d in Proceedings of the the 2nd ACM Conference on\nRecommender Systems, 2008.\n[58] Y. Koren, \u201cFactorization meets the neighborhood: a multifaceted\ncollaborative filtering model,\u201d in Proceedings of the\n14th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining (KDD '08), pp. 426-434, Las\nVegas, Nev, USA, August 2008.\n[59] M. R. McLaughlin and J. L. Herlocker, \u201cA collaborative\nfiltering algorithm and evaluation metric that accurately\nmodel the user experience,\u201d in Proceedings of 27th Annual\nInternational ACM SIGIR Conference on Research and Development\nin Information Retrieval (SIGIR '04), pp. 329-336,\nSheffield, UK, 2004.\n[60] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl,\n\u201cEvaluating collaborative filtering recommender systems,\u201d\nACM Transactions on Information Systems, vol. 22, no. 1, pp.\n5-53, 2004.\n[61] G. Salton and M. McGill, Introduction to Modern Information\nRetrieval, McGraw-Hill, New York, NY, USA, 1983.</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"GEN_REFERENCES\">[62] G. Karypis, \u201cEvaluation of item-based top-N recommendation\nalgorithms,\u201d in Proceedings of the International Conference\non Information and Knowledge Management (CIKM '01),\npp. 247-254, Atlanta, Ga, USA, November 2001.\n[63] M. Deshpande and G. Karypis, \u201cItem-based top-N recommendation\nalgorithms,\u201d ACM Transactions on Information\nSystems, vol. 22, no. 1, pp. 143-177, 2004.\n[64] J. L. Herlocker, J. A. Konstan, A. Borchers, and J. Riedl,\n\u201cAn algorithmic framework for performing collaborative\nfiltering,\u201d in Proceedings of the Conference on Research and\nDevelopment in Information Retrieval (SIGIR '99), pp. 230237,\n1999.\n[65] D. Lemire, \u201cScale and translation invariant collaborative\nfiltering systems,\u201d Information Retrieval, vol. 8, no. 1, pp.\n129-150, 2005.\n[66] X. Su, T. M. Khoshgoftaar, X. Zhu, and R. Greiner,\n\u201cImputation-boosted collaborative filtering using machine\nlearning classifiers,\u201d in Proceedings of the 23rd Annual ACM\nSymposium on Applied Computing (SAC '08), pp. 949-950,\nCeara\u00b4 Fortaleza, Brazil, March 2008.\n[67] R. J. A. Little, \u201cMissing-data adjustments in large surveys,\u201d\nJournal of Business &amp; Economic Statistics, vol. 6, no. 3, pp.\n287-296, 1988.\n[68] D. B. Rubin, Multiple Imputation for Nonresponse in Surveys,\nJohn Wiley &amp; Sons, New York, NY, USA, 1987.\n[69] S. A. Goldman and M. K. Warmuth, \u201cLearning binary\nrelations using weighted majority voting,\u201d Machine Learning,\nvol. 20, no. 3, pp. 245-271, 1995.\n[70] A. Nakamura and N. Abe, \u201cCollaborative filtering using\nweighted majority prediction algorithms,\u201d in Proceedings of\nthe 15th International Conference on Machine Learning (ICML\n'98), 1998.\n[71] C. Basu, H. Hirsh, and W. Cohen, \u201cRecommendation as\nclassification: using social and content-based information\nin recommendation,\u201d in Proceedings of the 15th National\nConference on Artificial Intelligence (AAAI '98), pp. 714-720,\nMadison, Wis, USA, July 1998.\n[72] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks\nof Plausible Inference, Morgan Kaufmann, San Francisco,\nCalif, USA, 1988.\n[73] B. Shen, X. Su, R. Greiner, P. Musilek, and C. Cheng, \u201cDiscriminative\nparameter learning of general Bayesian network\nclassifiers,\u201d in Proceedings of the 15th IEEE International\nConference on Tools with Artificial Intelligence, pp. 296-305,\nSacramento, Calif, USA, November 2003.\n[74] N. Friedman, D. Geiger, and M. Goldszmidt, \u201cBayesian\nnetwork classifiers,\u201d Machine Learning, vol. 29, no. 2-3, pp.\n131-163, 1997.\n[75] D. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite,\nand C. Kadie, \u201cDependency networks for inference, collaborative\nfiltering, and data visualization,\u201d Journal of Machine\nLearning Research, vol. 1, no. 1, pp. 49-75, 2001.\n[76] J. Han and M. Kamber, Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2001.\n[77] X. Su, M. Kubat, M. A. Tapia, and C. Hu, \u201cQuery size\nestimation using clustering techniques,\u201d in Proceedings of\nthe 17th International Conference on Tools with Artificial\nIntelligence (ICTAI '05), pp. 185-189, Hong Kong, November\n2005.\n[78] J. B. MacQueen, \u201cSome methods for classification and\nanalysis of multivariate observations,\u201d in Proceedings of the\n5th Symposium on Math, Statistics, and Probability, pp. 281297,\nBerkeley, Calif, USA, 1967.</zone>\n  <zone label=\"GEN_OTHER\">17</zone>\n  <zone label=\"GEN_REFERENCES\">[79] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, \u201cA density-based\nalgorithm for discovering clusters in large spatial databases\nwith noise,\u201d in Proceedings of the International Conference on\nKnowledge Discovery and Data Mining (KDD '96), 1996.\n[80] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and J. Sander,\n\u201cOPTICS: ordering points to identify the clustering structure,\u201d\nin Proceedings of ACM SIGMOD Conference, pp. 49-60,\nJune 1999.\n[81] T. Zhang, R. Ramakrishnan, and M. Livny, \u201cBIRCH: an\nefficient data clustering method for very large databases,\u201d in\nProceedings of the ACM SIGMOD International Conference\non Management of Data, vol. 25, pp. 103-114, Montreal,\nCanada, June 1996.\n[82] S. Geman and D. Geman, \u201cStochastic relaxation, Gibbs\ndistributions and the Bayesian restoration of images,\u201d IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol.\n6, no. 6, pp. 721-741, 1984.\n[83] T. Hofmann and J. Puzicha, \u201cLatent class models for collaborative\nfiltering,\u201d in Proceedings of the 16th International Joint\nConference on Artificial Intelligence (IJCAI '99), pp. 688-693,\n1999.\n[84] A. Ng and M. Jordan, \u201cPEGASUS: a policy search method\nfor large MDPs and POMDPs,\u201d in Proceedings of the 16th\nConference on Uncertainty in Artificial Intelligence (UAI '00),\nStanford, Calif, USA, 2000.\n[85] A. P. Dempster, N. M. Laird, and D. B. Rubin, \u201cMaximum\nlikelihood from incomplete data via the EM algorithm,\u201d\nJournal of the Royal Statistical Society. Series B, vol. 39, no.\n1, pp. 1-38, 1977.\n[86] D. M. Pennock, E. Horvitz, S. Lawrence, and C. L. Giles,\n\u201cCollaborative filtering by personality diagnosis: a hybrid\nmemory- and model-based approach,\u201d in Proceedings of the\n16th Conference on Uncertainty in Artificial Intelligence (UAI\n'00), pp. 473-480, 2000.\n[87] S. Vucetic and Z. Obradovic, \u201cCollaborative filtering using\na regression-based approach,\u201d Knowledge and Information\nSystems, vol. 7, no. 1, pp. 1-22, 2005.\n[88] D. Lemire and A. Maclachlan, \u201cSlope one predictors for\nonline rating-based collaborative filtering,\u201d in Proceedings of\nthe SIAM Data Mining Conference (SDM '05), 2005.\n[89] R. E. Bellman, Dynamic Programming, Princeton University\nPress, Princeton, NJ, USA, 1962.\n[90] R. A. Howard, Dynamic Programming and Markov Processes,\nMIT Press, Cambridge, Mass, USA, 1960.\n[91] R. S. Sutton and A. G. Barto, Reinforcement Learning: An\nIntroduction, MIT Press, Cambridge, Mass, USA, 1998.\n[92] M. Hauskrecht, \u201cIncremental methods for computing\nbounds in partially observable Markov decision processes,\u201d\nin Proceedings of the 14th National Conference on Artificial\nIntelligence (AAAI '97), pp. 734-739, Providence, RI, USA,\nJuly 1997.\n[93] P. Poupart and C. Boutilier, \u201cVDCBPI: an approximate\nscalable algorithm for large POMDPs,\u201d in Proceedings of the\n18th Annual Conference on Neural Information Processing\nSystems (NIPS '04), 2004.\n[94] M. Kearns, Y. Mansour, and A. Y. Ng, \u201cA sparse sampling\nalgorithm for near-optimal planning in large Markov decision\nprocesses,\u201d Machine Learning, vol. 49, no. 2-3, pp. 193208,\n2002.\n[95] T. Hofmann, \u201cUnsupervised learning by probabilistic latent\nsemantic analysis,\u201d Machine Learning, vol. 42, no. 1-2, pp.\n177-196, 2001.</zone>\n  <zone label=\"GEN_OTHER\">18</zone>\n  <zone label=\"GEN_REFERENCES\">[96] EachMovie dataset, http://www.grouplens.org/node/76.\n[97] B. Marlin, \u201cModeling user rating profiles for collaborative\nfiltering,\u201d in Neural Information Processing Systems, 2003.\n[98] B. Marlin, Collaborative filtering, a machine learning perspective,\nM.S. thesis, Department of Computer Science,\nUniversity of Toronto, 2004.\n[99] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent Dirichlet\nallocation,\u201d Journal of Machine Learning Research, vol. 3, no.\n4-5, pp. 993-1022, 2003.\n[100] W. W. Cohen, R. E. Schapire, and Y. Singer, \u201cLearning to\norder things,\u201d Journal of Artificial Intelligence Research, vol.\n10, pp. 243-270, 1999.\n[101] X. Fu, J. Budzik, and K. J. Hammond, \u201cMining navigation\nhistory for recommendation,\u201d in Proceedings of the International\nConference on Intelligent User Interfaces (IUI '00), pp.\n106-112, 2000.\n[102] C. W. K. Leung, S. C. F. Chan, and F. L. Chung, \u201cA\ncollaborative filtering framework based on fuzzy association\nrules and multi-level similarity,\u201d Knowledge and Information\nSystems, vol. 10, no. 3, pp. 357-381, 2006.\n[103] D. Nikovski and V. Kulev, \u201cInduction of compact decision\ntrees for personalized recommendation,\u201d in Proceedings of the\nACM Symposium on Applied Computing, vol. 1, pp. 575-581,\nDijon, France, April 2006.\n[104] C. C. Aggarwal, J. L. Wolf, K. Wu, and P. S. Yu, \u201cHorting\nhatches an egg: a new graph-theoretic approach to collaborative\nfiltering,\u201d in Proceedings of the 5th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data\nMining (KDD '99), pp. 201-212, 1999.\n[105] B. Marlin and R. S. Zemel, \u201cThe multiple multiplicative\nfactor model for collaborative filtering,\u201d in Proceedings of the\n21st International Conference on Machine Learning (ICML\n'04), pp. 576-583, Banff, Canada, July 2004.\n[106] M. E. Tipping and C. M. Bishop, \u201cProbabilistic principal\ncomponent analysis,\u201d Journal of the Royal Statistical Society.\nSeries B, vol. 21, no. 3, pp. 611-622, 1999.\n[107] G. Taka\u00b4cs, I. Pila\u00b4szy, B. Ne\u00b4meth, and D. Tikk, \u201cInvestigation\nof various matrix factorization methods for large recommender\nsystems,\u201d in Proceedings of the IEEE International\nConference on Data Mining Workshops (ICDM '08), pp. 553562,\nPisa, Italy, December 2008.\n[108] J. Wang, S. Robertson, A. P. de Vries, and M. J. T. Reinders,\n\u201cProbabilistic relevance ranking for collaborative filtering,\u201d\nInformation Retrieval, vol. 11, no. 6, pp. 477-497, 2008.\n[109] J. Wang, A. P. de Vries, and M. J. T. Reinders, \u201cUnified\nrelevance models for rating prediction in collaborative\nfiltering,\u201d ACM Transactions on Information Systems, vol. 26,\nno. 3, pp. 1-42, 2008.\n[110] M. J. Pazzani, \u201cA framework for collaborative, content-based\nand demographic filtering,\u201d Artificial Intelligence Review, vol.\n13, no. 5-6, pp. 393-408, 1999.\n[111] T. Zhu, R. Greiner, and G. Haubl, \u201cLearning a model of a\nweb user's interests,\u201d in Proceedings of the 9th International\nConference on User Modeling (UM '03), vol. 2702, pp. 65-75,\nJohnstown, Pa, USA, June 2003.\n[112] M. Pazzani and D. Billsus, \u201cLearning and revising user\nprofiles: the identification of interesting web sites,\u201d Machine\nLearning, vol. 27, no. 3, pp. 313-331, 1997.\n[113] M. K. Condiff, D. D. Lewis, D. Madigan, and C. Posse,\n\u201cBayesian mixed-effects models for recommender systems,\u201d\nin Proceedings of ACM SIGIR Workshop of Recommender\nSystems: Algorithm and Evaluation, 1999.</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"GEN_REFERENCES\">[114] B. Krulwich, \u201cLifestyle finder: intelligent user profiling\nusing large-scale demographic data,\u201d Artificial Intelligence\nMagazine, vol. 18, no. 2, pp. 37-45, 1997.\n[115] R. Burke, \u201cHybrid recommender systems: survey and experiments,\u201d\nUser Modelling and User-Adapted Interaction, vol. 12,\nno. 4, pp. 331-370, 2002.\n[116] R. H. Guttman, Merchant differentiation through integrative\nnegotiation in agent-mediated electronic commerce, M.S. thesis,\nSchool of Architecture and Planning, MIT, 1998.\n[117] M. Balabanovic\u00b4 and Y. Shoham, \u201cContent-based, collaborative\nrecommendation,\u201d Communications of the ACM, vol. 40,\nno. 3, pp. 66-72, 1997.\n[118] X. Su, R. Greiner, T. M. Khoshgoftaar, and X. Zhu, \u201cHybrid\ncollaborative filtering algorithms using a mixture of experts,\u201d\nin Proceedings of the IEEE/WIC/ACM International Conference\non Web Intelligence (WI '07), pp. 645-649, Silicon Valley,\nCalif, USA, November 2007.\n[119] A. E. Gelfand and A. F. M. Smith, \u201cSampling-based\napproaches to calculating marginal densities,\u201d Journal of the\nAmerican Statistical Association, vol. 85, pp. 398-409, 1990.\n[120] B. M. Sarwar, J. A. Konstan, A. Borchers, J. Herlocker,\nB. Miller, and J. Riedl, \u201cUsing filtering agents to improve\nprediction quality in the grouplens research collaborative\nfiltering system,\u201d in Proceedings of the ACM Conference on\nComputer Supported Cooperative Work (CSCW '98), pp. 345354,\nSeattle, Wash, USA, 1998.\n[121] R. J. Mooney and L. Roy, \u201cContent-based book recommendation\nusing learning for text categorization,\u201d in Proceedings\nof the Workshop on Recommender Systems: Algorithms and\nEvaluation (SIGIR '99), Berkeley, Calif, USA, 1999.\n[122] J. A. Delgado, Agent-based information filtering and recommender\nsystems on the internet, Ph.D. thesis, Nagoya Institute\nof Technology, February 2000.\n[123] R. E. Schapire, \u201cA brief introduction to boosting,\u201d in\nProceedings of the 16th International Joint Conference on\nArtificial Intelligence (IJCAI '99), pp. 1401-1405, 1999.\n[124] B. Smyth and P. Cotter, \u201cA personalized TV listings service\nfor the digital TV age,\u201d in Proceedings of the 19th International\nConference on Knowledge-Based Systems and Applied Artificial\nIntelligence (ES '00), vol. 13, pp. 53-59, Cambridge, UK,\nDecember 2000.\n[125] M. Balabanovic\u00b4, \u201cExploring versus exploiting when learning\nuser models for text recommendation,\u201d User Modelling and\nUser-Adapted Interaction, vol. 8, no. 1-2, pp. 71-102, 1998.\n[126] A. Popescul, L. H. Ungar, D. M. Pennock, and S. Lawrence,\n\u201cProbabilistic models for unified collaborative and contentbased\nrecommendation in sparse-data environments,\u201d in\nProceedings of the 17th Conference in Uncertainty in Artificial\nIntelligence (UAI '01), pp. 437-444, 2001.\n[127] CiteSeer ResearchIndex, digital library of computer science\nresearch papers, http://citeseer.ist.psu.edu.\n[128] Y. Y. Yao, \u201cMeasuring retrieval effectiveness based on user\npreference of documents,\u201d Journal of the American Society for\nInformation Science, vol. 46, no. 2, pp. 133-145, 1995.\n[129] R. Sinha and K. Swearingen, \u201cComparing recommendations\nmade by online systems and friends,\u201d in Proceedings of the\nDELOS-NSF Workshop on Personalization and Recommender\nSystems in Digital Libraries, 2001.\n[130] S. M. McNee, J. Riedl, and J. A. Konstan, \u201cAccurate is not\nalways good: how accuracy metrics have hurt recommender\nsystems,\u201d in Proceedings of the ACM Conference on Human\nFactors in Computing Systems (CHI '06), 2006.</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in Artificial Intelligence</zone>\n  <zone label=\"GEN_REFERENCES\">[131] D. J. Hand and R. J. Till, \u201cA simple generalisation of the\narea under the ROC curve for multiple class classification\nproblems,\u201d Machine Learning, vol. 45, no. 2, pp. 171-186,\n2001.\n[132] F. Provost and P. Domingos, \u201cWell-trained pets: improving\nprobability estimation trees,\u201d CDER Working Paper IS-0004,\nStern School of Business, New York University, 2000.\n[133] Z. Huang, W. Chung, and H. Chen, \u201cA graph model for Ecommerce\nrecommender systems,\u201d Journal of the American\nSociety for Information Science and Technology, vol. 55, no. 3,\npp. 259-274, 2004.</zone>\n  <zone label=\"GEN_OTHER\">19</zone>\n  <zone label=\"BODY_CONTENT\">Advances</zone>\n  <zone label=\"GEN_OTHER\">in</zone>\n  <zone label=\"MET_TYPE\">M ultimedia</zone>\n  <zone label=\"GEN_OTHER\">Hindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014</zone>\n  <zone label=\"MET_BIB_INFO\">Advances in</zone>\n  <zone label=\"BODY_HEADING\">Fuzzy\nSystems</zone>\n  <zone label=\"BODY_CONTENT\">Hindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014</zone>\n  <zone label=\"BODY_CONTENT\">Journal of\nComputer\nNetworks\nand Communicatio</zone>\n  <zone label=\"GEN_OTHER\">ns</zone>\n  <zone label=\"BODY_CONTENT\">Hindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014</zone>\n  <zone label=\"MET_BIB_INFO\">Journal of\nIndustrial Engineering</zone>\n  <zone label=\"GEN_OTHER\">The Scienticfi\nWorld Journal\nHindawi Publishing Corporation\nht p:/ www.hindawi.com\nVolume 2014\nInternational Journal of</zone>\n  <zone label=\"BODY_CONTENT\">Distrib\nuted</zone>\n  <zone label=\"GEN_OTHER\">Sensor Networks</zone>\n  <zone label=\"BODY_CONTENT\">Hindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014</zone>\n  <zone label=\"BODY_CONTENT\">Hindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014</zone>\n  <zone label=\"GEN_OTHER\">Hindawi Publishing Corporation\nht p:/ w w.hindawi.com\nVolume 2014</zone>\n  <zone label=\"BODY_CONTENT\">Applied\nComputational\nIntelligence and Soft\nComputing\nModelling\nSim\nulation\n&amp;</zone>\n  <zone label=\"GEN_OTHER\">in</zone>\n  <zone label=\"BODY_CONTENT\">Engineering</zone>\n  <zone label=\"GEN_OTHER\">Hindawi Publishing Corporation\nht p:/ www.hindawi.com\nVolume 2014\n\u2009Advances\u2009in\u2009</zone>\n  <zone label=\"BODY_CONTENT\">Articfiial\nIntelligence</zone>\n  <zone label=\"GEN_OTHER\">Hindawi\u2009Publishing\u2009Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"BODY_CONTENT\">Volume\u20092014\nSubmit\nyour manuscr ipts</zone>\n  <zone label=\"GEN_OTHER\">at\nhttp://www.hindawi.com\nInternational Journal of\nBiomedical Imaging\nAdvances in</zone>\n  <zone label=\"BODY_CONTENT\">Artificial</zone>\n  <zone label=\"GEN_OTHER\">Neural Systems</zone>\n  <zone label=\"BODY_CONTENT\">Advances in\nComputer\nHindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"GEN_OTHER\">En</zone>\n  <zone label=\"BODY_CONTENT\">gineering</zone>\n  <zone label=\"MET_DATES\">Volume 2014</zone>\n  <zone label=\"MET_BIB_INFO\">International Journal of</zone>\n  <zone label=\"BODY_CONTENT\">Computer Games\nTechnology\nHindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014</zone>\n  <zone label=\"GEN_OTHER\">Hindawi Publishing Corporation</zone>\n  <zone label=\"MET_BIB_INFO\">ht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014</zone>\n  <zone label=\"GEN_OTHER\">Hindawi Publishing Corporation</zone>\n  <zone label=\"MET_BIB_INFO\">ht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014\nVolume 2014</zone>\n  <zone label=\"BODY_CONTENT\">Advances in\nSoftware\nHindawi Publishing Corporation\nht p:/ www.hindawi.com\nEngineering</zone>\n  <zone label=\"GEN_OTHER\">International Journal of</zone>\n  <zone label=\"BODY_CONTENT\">Reconfigurable\nComputing\nJournal of\nRobotics\nHindawi Publishing Corporation\nht p:/ www.hindawi.com\nAdvances in</zone>\n  <zone label=\"GEN_OTHER\">Human-Computer\nInteraction\nHindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"BODY_CONTENT\">Computational\nIntelligence\nand</zone>\n  <zone label=\"GEN_OTHER\">Neuroscience</zone>\n  <zone label=\"MET_BIB_INFO\">Hindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"GEN_OTHER\">Journal of</zone>\n  <zone label=\"BODY_CONTENT\">Electrical\nand\nComputer</zone>\n  <zone label=\"GEN_OTHER\">En\ngineering\nHindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014\nVolume 2014\nVolume 2014</zone>\n  <zone label=\"MET_BIB_INFO\">Hindawi Publishing Corporation\nht p:/ www.hindawi.com</zone>\n  <zone label=\"MET_DATES\">Volume 2014\nVolume 2014</zone>\n</document>"}