=======2023.May.6=======
Need to convert the pdf into a single string. And then can feed to the API.
Better parse the paper well before sending.

=======2023.May.7=======
Use CERMINE to parse the paper (PDF->string), the string contains all the information about the paper, like author and
abstract, citation, etc. I think it's good enough so far to proceed to this next stage.

my paper_reader are practically dataloaders. So far I just want to realize embed one singe file, so my dataloaders will
return a string of one file at a time.

Met problem: Open Ai's embedding can only accept 1024 tokens at one time, so it is not possible to embed a whole paper.
Solution1: embed by chunks?
Solution2: embed summary?
Solution3: embed metadata?
Most convincing solution: embed each section, and weighted sum up based on their importance
my solution today: cut into segments of equal length, and embed it and take average

=======2023.May.9=======
http://arxiv.org/abs/2303.13340
This paper gives me an insight: We can use a sliding window plus unweighted average sum pooling technique to see if it
is good.
Other possibilities (explore later):
1. represent a research paper into a set of embeddings and compare set to set's distance (you decide how)
2. imagine each embedding is a point in the space, then taking average is just a way of finding the mid point of the
set of points, then it's not resilient to outliers.
3. weighted average
As Aviv said, now I need the basic functionalities, MVP, later I will revisit.

I was going to start with word2vec, but word2vec is an embedding for word, so if we want to embed sentence, we might
need to using pooling techiques.
Other good embedding for SENTENCES:
Bert, USE (Universal Sentence Encoder), look it up later.

So far i write my openai api key in the code, later need to change it to a path "openai.api_key = <API-KEY>"

So without too much trouble, just need to add the payment methods, and generate a personal private key on the openai
website, this whole thing already works.
Each paper takes about 30 seconds to get the embedding, and about 0.02 dollar. Seems like there is a 18 dollar free
trial everyday?

=======2023.May.10=======
*********************************************
*******                              ********
*******  First edition summarize     ********
*******                              ********
*********************************************
In the first edition, I only set up the openAI embedding method. This embedding method will use a paper reader to get an
attribute dictionary for the paper, and since so far there is only "content" in the dict, we will only operate on the
content attribute.
Because the openAI embedding can only take 8000 tokens at a time for embedding, my solution sofar is using a sliding
window that have half overlap, and take the unweighted average of each embedded vectors. This can be modified later too.

Sadly, seems like semetic scholar have already parsed data with embeddings, so I will first try to use it's embeddings,
this project will be frozen too.
